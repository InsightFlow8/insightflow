---
title: "Imba_data EDA"
author: "Zhenyu Zhang"
date: "2025-06-13"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(data.table)
library(purrr)
library(skimr)
library(tidyverse)
library(scales)
library(knitr)
library(lubridate)
library(DT)
library(ggplot2)
library(scales)
library(corrplot)
library(lubridate)
```


```{r load locol-data, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
orders                <- read_csv("orders.csv", show_col_types = FALSE)
products              <- read_csv("products.csv", show_col_types = FALSE)
aisles                <- read_csv("aisles.csv", show_col_types = FALSE)
departments           <- read_csv("departments.csv", show_col_types = FALSE)
order_products_prior  <- read_csv("order_products_prior.csv.gz", show_col_types = FALSE)
order_products_train  <- read_csv("order_products_train.csv.gz", show_col_types = FALSE)
```

---
1. raw data pre check
```{r pre_check, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# consistency check Primary key uniqueness check
# prior
order_products_prior %>% 
  summarise(total = n(), 
            distinct_pairs = n_distinct(paste(order_id, product_id, sep = "_")))

# train
order_products_train %>% 
  summarise(total = n(), 
            distinct_pairs = n_distinct(paste(order_id, product_id, sep = "_")))
```
```{r pre_check_2, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Foreign key integrity check
# prior
order_products_prior %>% anti_join(orders,    by = "order_id")   %>% count()   # 应为 0
order_products_prior %>% anti_join(products,  by = "product_id") %>% count()   # 应为 0

# train
order_products_train %>% anti_join(orders,    by = "order_id")   %>% count()   # 应为 0
order_products_train %>% anti_join(products,  by = "product_id") %>% count()   # 应为 0
```


```{r pre_check_3, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Put all tables into a list
tables <- list(
  orders                 = orders,
  order_products_prior   = order_products_prior,
  order_products_train   = order_products_train,
  products               = products,
  aisles                 = aisles,
  departments            = departments
)

# 1. Total number of missing values in each table
map_int(tables, ~ sum(is.na(.))) %>% 
  enframe(name = "table", value = "n_missing") %>% 
  knitr::kable(caption = "Total number of missing values")

# 2. Missing rate by column in each table
map(tables, ~ map_dbl(., ~ mean(is.na(.)))) %>% 
  map_df(~ .x, .id = "table") %>% 
  pivot_longer(-table, names_to = "column", values_to = "missing_rate") %>% 
  filter(missing_rate > 0) %>% 
  arrange(desc(missing_rate)) %>% 
  knitr::kable(
    caption = "Missing rate by column in each table (only missing columns are listed)",
    digits = 3
  )

```

```{r pre_check_4, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# 3. Calculate the number and rate of missing data for each table and column
missing_detail <- map_df(
  tables,
  ~ tibble(
      column       = names(.x),
      n_missing    = colSums(is.na(.x)),
      missing_rate = colMeans(is.na(.x))
    ),
  .id = "table"
) %>%
  filter(n_missing > 0) %>%
  arrange(table, desc(missing_rate))

missing_detail %>%
  knitr::kable(
    caption = "Detailed Missing Data by Table and Column",
    digits = c(0, 0, 3),
    col.names = c("Table", "Column", "Missing Count", "Missing Rate")
  )

```

```{r pre_check_5, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
orders_cleaned <- orders %>% 
  drop_na(days_since_prior_order)
```




```{r EDA for order table, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Display the first few rows of the tables
# head(orders)

# Use `spec()` and/or `skimr::skim()` to retrieve the full column specification for this data
spec(orders)
skimr::skim(orders)
spec(products)
spec(aisles)
spec(departments)
spec(order_products_prior)
spec(order_products_train)

```
---

2. Single table EDA（Explore Data Analysis）

2.0 准备工作
type conversion [把离散的数字列变成因子或整数，便于后续绘图与汇总]
```{r type conversion, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
orders <- orders %>% 
  mutate(
    order_dow          = factor(order_dow, levels = 0:6,
                                labels = c("Sun","Mon","Tue","Wed","Thu","Fri","Sat")),
    order_hour_of_day  = as.integer(order_hour_of_day),
    eval_set           = factor(eval_set, levels = c("prior","train","test"))
  )


orders_cleaned <- orders_cleaned %>% 
  mutate(
    order_dow          = factor(order_dow, levels = 0:6,
                                labels = c("Sun","Mon","Tue","Wed","Thu","Fri","Sat")),
    order_hour_of_day  = as.integer(order_hour_of_day),
    eval_set           = factor(eval_set, levels = c("prior","train","test"))
  )


order_products_prior <- order_products_prior %>% mutate_all(~ ifelse(is.na(.), ., .))
order_products_train <- order_products_train

products   <- products   %>% mutate(across(c(product_id, aisle_id, department_id), as.integer))
aisles     <- aisles     %>% mutate(aisle_id = as.integer(aisle_id))
departments<- departments%>% mutate(department_id = as.integer(department_id))

```


```{r orders, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
orders %>% 
  summarise(
    total_orders = n(),
    total_users  = n_distinct(user_id)
  )


```


2.1 订单 (orders)
按星期 & 小时分布
```{r orders distribution, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Order distribution by day of the week
orders_cleaned %>% 
  count(order_dow) %>% 
  ggplot(aes(order_dow, n)) +
    geom_col() +
    labs(
      title = "Order distribution by day of the week",
      x = NULL, y = "Order Number"
    ) +
    theme_minimal()

# Order distribution by hour
orders_cleaned %>% 
  count(order_hour_of_day) %>% 
  ggplot(aes(order_hour_of_day, n)) +
    geom_col() +
    scale_x_continuous(breaks = seq(0,23,by=2)) +
    labs(
      title = "Order distribution by hour",
      x = "Hour", y = "Order Number"
    ) +
    theme_minimal()

```

复购周期分布
```{r repurchase cycle distribution, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
orders_cleaned %>% 
  filter(!is.na(days_since_prior_order)) %>% 
  ggplot(aes(days_since_prior_order)) +
    geom_histogram(binwidth = 1) +
    scale_x_continuous(limits = c(0, 30)) +    # 截取前两个月更清晰
    labs(
      title = "Distribution of days between repurchases",
      x = "Days since last order", y = "Frequency"
    ) +
    theme_minimal()

```

用户下单次数分布
```{r distribution of order times, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
orders_cleaned %>% 
  count(user_id, name = "order_count") %>% 
  ggplot(aes(order_count)) +
    geom_histogram(
      breaks = seq(0, 100, by = 10),
      closed = "right",
      color  = "white"
    ) +
    # linear X axis with 10-unit ticks
    scale_x_continuous(
      breaks = seq(0, 100, by = 10),
      labels = comma_format()
    ) +
    # log10 on Y axis
    scale_y_continuous(
      trans  = "log10",
      labels = comma_format()
    ) +    labs(
      title = "Distribution of Orders Per User",
      x     = "Total Orders per User",
      y     = "Number of Users (log scale)"
    ) +
    theme_minimal()


```


2.2 订单–产品 (order_products)
```{r order_products1, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# 合并示例
order_products <- bind_rows(
  order_products_prior  %>% mutate(src = "prior"),
  order_products_train  %>% mutate(src = "train")
)

order_products %>% 
  count(order_id, name="items_per_order") %>% 
  ggplot(aes(items_per_order)) +
    # bins of size 10: [0–10),[10–20)… up to 100
    geom_histogram(
      breaks = seq(0,100,by=10), 
      closed = "right", 
      color  = "white"
    ) +
    scale_x_continuous(
      breaks = seq(0,100,by=10),
      labels = seq(0,100,by=10)
    ) +
    scale_y_continuous(
      trans  = "log10",
      labels = scales::comma_format()
    ) +
    labs(
      title = "每笔订单的商品数量分布",
      x     = "商品数量",
      y     = "订单数 (log scale)"
    ) +
    theme_minimal()

```

```{r order_products2, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# how often items land in each cart position
order_products %>% 
  count(add_to_cart_order) %>% 
  filter(add_to_cart_order <= 20) %>%   # 限制到前20个位置
  ggplot(aes(add_to_cart_order, n)) +
    geom_col() +
    labs(
      title = "加入购物车顺序分布（前20位）",
      x     = "加入顺序",
      y     = "频次"
    ) +
    theme_minimal()

```


```{r order_products3, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
order_products %>% 
  group_by(product_id) %>% 
  summarise(
    avg_pos = mean(add_to_cart_order),
    count   = n(),
    .groups = "drop"
  ) %>% 
  filter(count >= 1000) %>%    # 只看销量够大的产品
  slice_min(avg_pos, n = 10) %>% 
  left_join(products, by="product_id") %>% 
  ggplot(aes(fct_reorder(product_name, avg_pos), avg_pos)) +
    geom_col() +
    coord_flip() +
    labs(
      title = "平均加入购物车顺序最靠前的10种产品",
      x     = NULL,
      y     = "平均加入顺序"
    ) +
    theme_minimal()


```






```{r order_products4, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# overall spread of reorder_rate across products
order_products %>% 
  group_by(product_id) %>% 
  summarise(
    reorder_rate = mean(reordered),
    sales        = n(),
    .groups      = "drop"
  ) %>% 
  ggplot(aes(reorder_rate)) +
    geom_histogram(bins = 50) +
    labs(
      title = "产品层面复购率分布",
      x     = "复购率",
      y     = "产品数"
    ) +
    theme_minimal()

# Top-10 复购率最高产品
order_products %>% 
  group_by(product_id) %>% 
  summarise(
    reorder_rate = mean(reordered),
    sales        = n(),
    .groups      = "drop"
  ) %>% 
  filter(sales >= 1000) %>% 
  slice_max(reorder_rate, n = 10) %>% 
  left_join(products, by="product_id") %>% 
  ggplot(aes(fct_reorder(product_name, reorder_rate), reorder_rate)) +
    geom_col() +
    coord_flip() +
    labs(
      title = "复购率最高的 10 种产品",
      x     = NULL,
      y     = "复购率"
    ) +
    theme_minimal()


# Prior vs Train 全局复购率
order_products %>% 
  group_by(src) %>% 
  summarise(
    reorder_rate = mean(reordered),
    .groups      = "drop"
  ) %>% 
  ggplot(aes(src, reorder_rate)) +
    geom_col() +
    labs(
      title = "Prior vs Train 全局复购率对比",
      x     = NULL,
      y     = "复购率"
    ) +
    theme_minimal()


```


2.3 产品维度
各部门 & 货架产品数
```{r order_products, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
products %>% 
  count(department_id, name = "n") %>% 
  left_join(departments, by="department_id") %>% 
  ggplot(aes(fct_reorder(department, n), n)) +
    geom_col() +
    coord_flip() +
    labs(title="各部门产品数", x=NULL, y="产品数") +
    theme_minimal()

products %>% 
  count(aisle_id, name="n") %>% 
  left_join(aisles, by="aisle_id") %>% 
  slice_max(n, n=10) %>% 
  ggplot(aes(fct_reorder(aisle, n), n)) +
    geom_col() +
    coord_flip() +
    labs(title="Top10 货架的产品数", x=NULL, y="产品数") +
    theme_minimal()


```
```{r define-prod-with-aisle, echo=FALSE, message=FALSE}
prod_with_aisle <- products %>% 
  left_join(aisles, by = "aisle_id")
```

```{r order_products_missing_aisle, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# 1. 计算 missing‐aisle ratio
missing_stats <- products %>%
  # bring in the aisle names
  left_join(aisles, by = "aisle_id") %>%
  # now we can filter on the aisle column
  filter(aisle == "missing") %>%
  summarise(
    Missing_Count = n(),
    Total_Products = nrow(products),
    Missing_Pct = percent(Missing_Count / Total_Products)
  )

missing_stats %>% 
  kable(caption = "Products with aisle == 'missing'")

# 2. Filter out "missing" and plot Top 10
prod_with_aisle %>%
  filter(aisle != "missing") %>%        # drop the missing bucket
  count(aisle, name = "n") %>%          # count products by aisle
  slice_max(n, n = 10) %>%              # select top 10 by count
  ggplot(aes(fct_reorder(aisle, n), n)) +
    geom_col() +
    coord_flip() +
    labs(
      title = "Top 10 货架的产品数（排除 'missing'）",
      x     = NULL,
      y     = "产品数"
    ) +
    theme_minimal()
```


复购率最高 TopN 产品
```{r order_products_TopN, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
order_products %>% 
  group_by(product_id) %>% 
  summarise(
    sales = n(),
    reorder_rate = mean(reordered)
  ) %>% 
  filter(sales > 1000) %>%             # 过滤销量太小的噪声
  slice_max(reorder_rate, n=10) %>%    # 复购率 Top10
  left_join(products, by="product_id") %>% 
  ggplot(aes(fct_reorder(product_name, reorder_rate), reorder_rate)) +
    geom_col() +
    coord_flip() +
    labs(
      title="复购率最高的 10 种产品",
      x=NULL, y="复购率"
    ) +
    theme_minimal()

```


3. 多表联合分析
3.1 热门部门 & 通道

```{r Popular Departments & Channels, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
order_products %>% 
  count(product_id, name="sales") %>% 
  left_join(products,    by="product_id") %>% 
  left_join(departments,by="department_id") %>% 
  group_by(department) %>% 
  summarise(
    total_sales   = sum(sales),
    avg_reorder   = mean(order_products$reordered[order_products$product_id %in% product_id])
  ) %>% 
  arrange(desc(total_sales)) %>% 
  slice_head(n=10)


```

3.2 用户购买行为分群（RFM）
```{r RFM, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# RFM 示例
rfm <- orders %>% 
  group_by(user_id) %>% 
  summarise(
    recency   = as.numeric(today() - max(order_number)),  # 若有具体日期字段请替换
    frequency = n(),
    monetary  = NA_real_                             # 无金额字段可留空或用 order_number 估算
  )

# 简易分群
rfm %>% 
  mutate(
    R_score = ntile(-recency, 5),
    F_score = ntile(frequency, 5)
  ) %>% 
  ggplot(aes(F_score, R_score)) +
    geom_jitter(alpha=0.3) +
    labs(
      title="用户 RFM 分布 (简易版)",
      x="Frequency Score", y="Recency Score"
    ) +
    theme_minimal()


```

RFM 基础知识
Recency (R)新近度 (R)：距离客户上次下单的时间。

Frequency (F) 频率 (F)：他们总共下单了多少单。

(Monetary (M) is left out here because we don’t have $-value fields.)

综合起来，RFM 用于识别以下群体：

R-score	      F-score	          Who these are
High        	High	            Champions (最佳!)
High        	Low	              New or About-to-Sleep 新客户或即将入驻（有时高新近度仅指首次下单）
Low	          High            	Loyal but Inactive 忠诚但不活跃（过去经常购买，但最近没有）
Low	          Low	              At Risk or Lost 处于风险或可能会流失





4. 时序与趋势
```{r time, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# 日级订单量
orders %>% 
  mutate(order_date = today() - days_since_prior_order) %>%  # 需真实日期列替换
  count(order_date) %>% 
  ggplot(aes(order_date, n)) +
    geom_line() +
    labs(title="日级订单量趋势", x=NULL, y="订单数") +
    theme_minimal()


```
5 月 26 日左侧的峰值是由于合成 `order_date` 的方式造成的：
`orders.csv` 中**实际上没有真正的日历日期**——只有
* `order_dow` (0-6)
* `order_hour_of_day`
* `days_since_prior_order`（上限为 30；不适用于首单）
通过执行
mutate(order_date = today() - days_since_prior_order)
所有 `days_since_prior_order == 30` 的订单（即所有超过 30 天的订单）映射到**一个单一日期**（今天减 30）。这会将数百万行数据集中到该日期，因此会出现巨大的峰值。

如何修复
1. 真实的订单时间戳 - 没有。。。
2. 使用起始日期锚点进行近似
    为每个用户的第一个订单选择一个任意的“锚点”日期，然后累计添加 `days_since_prior_order` 偏移量，以重建伪日历
    a. 先把每个用户的 NA（首单）当作 0 天间隔。
    b. 对每个用户按 order_number 累加这些天数，得到 cum_days。
    c. 假设「最后一次下单」对应今天（或一个指定的固定日期），则锚点 anchor_date = today() − max(cum_days)
    d. 最终日期 = anchor_date + cum_days。
    
```{r Pseudo-calendar, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# 以今天为“最后一次下单日”基准；也可改成 as.Date("2025-06-23")
base_date <- today()

orders_with_dates <- orders %>%
  arrange(user_id, order_number) %>% 
  group_by(user_id) %>% 
  mutate(
    # 将 NA（首单）视为 0 天
    days = replace_na(days_since_prior_order, 0),
    # 累积天数
    cum_days = cumsum(days),
    # 计算每个用户的锚点日期
    anchor_date = base_date - max(cum_days),
    # 重建伪订单日期
    order_date = anchor_date + cum_days
  ) %>% 
  ungroup() %>% 
  select(-days, -cum_days, -anchor_date)

# 看一下示例
orders_with_dates %>% 
  filter(user_id %in% c(1,2,3)) %>% 
  select(user_id, order_number, days_since_prior_order, order_date) %>% 
  head(10) %>% 
  knitr::kable()

```

```{r Trend Chart, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
orders_with_dates %>% 
  count(order_date) %>% 
  ggplot(aes(order_date, n)) +
    geom_line() +
    labs(
      title = "伪日历下的日级订单量趋势",
      x = NULL, y = "订单数"
    ) +
    theme_minimal()
```


锚定“上次订单 = 今天”会导致不正常的峰值 （不可行）

两种更简单的替代方案
A) 绘制 相对时间轴
```{r relative timeline, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
orders_rel <- orders %>%
  arrange(user_id, order_number) %>%
  group_by(user_id) %>%
  mutate(
    days     = replace_na(days_since_prior_order, 0),
    cum_days = cumsum(days)
  ) %>%
  ungroup()

# Only re-orders (drop cum_days == 0)
orders_rel %>%
  filter(cum_days > 0) %>%
  count(cum_days) %>%
  ggplot(aes(cum_days, n)) +
    geom_line() +
    labs(
      title = "Trend by Days Since First Order (Re-orders Only)",
      x     = "Days Since First Order",
      y     = "Re-order Count"
    ) +
    theme_minimal()

```



B) 锚定到**中点**或人为设定的“开始日期”
```{r artificial start date, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
base_date <- as.Date("2024-01-01")  # fixed start

orders_fake <- orders %>%
  arrange(user_id, order_number) %>%
  group_by(user_id) %>%
  mutate(
    days       = replace_na(days_since_prior_order, 0),
    cum_days   = cumsum(days),
    order_date = base_date + cum_days
  ) %>%
  ungroup()

# Only re-orders (drop the zero-day, i.e. first orders)
orders_fake %>%
  filter(order_date > base_date) %>%
  count(order_date) %>%
  ggplot(aes(order_date, n)) +
    geom_line() +
    labs(
      title = "Pseudo-calendar Trend (Re-orders Only, Jan 1 2024 start)",
      x     = "Order Date",
      y     = "Re-order Count"
    ) +
    theme_minimal()

```

这两张图展示的是每个用户在首次下单以后各个“天”上的复购次数，只统计了cum_days > 0（也就是第二次及以后的订单），从而剔除了那条“所有人首单集中在 0 天” 的巨大峰值

1. 30 天处的尖峰
    由于原始字段 days_since_prior_order 被 截断在 0–30 天，所有超过 30 天的实际复购都被当作 30 天来记录，所以在“30 天”位置出现一个异常大的峰值。但是当一个用户继续下第 4、5、6… 单时，会把一次次的“30 天”累加进。因此，cum_days 会很自然地跑到 60、90、120… 这些数值——对应用户在第 2、3、4… 次复购时，每次都“至少 30 天”后才买
2. 7 天为周期的波动
    可以看到除了 30 天主峰以外，图上还有许多小峰，大约间隔 7 天出现一次——这反映了用户有 每周购物 的行为习惯。
3. 随时间的衰减趋势
    随着自首次下单以来的天数增加，复购次数在整体上呈 指数型衰减，说明大多数用户会随着时间推移逐渐减少下单频率，只有少数“高忠诚度”用户会持续复购。
4. 前几天的上升
    在第 1–10 天内的曲线从零点上来会有一个上升期，说明很多用户喜欢在首次尝试后不久就进行第二次购买。

价值与应用

  周期性促销：可以针对“每周复购”的人群设计周期性营销（例如 7 日回购提醒）。
  生命周期管理：根据复购衰减曲线，识别“流失高风险期”（比如第 20–30 天）并推送激励。
  数据校准：30 天截断峰提醒我们，用此字段做时间分析时要小心截断偏差；如果需要，更细粒度的天数数据会更准确。
  数据限制提示：峰值都是“≥30 天”打包，因此无法区分真实是 31 天还是 90 天，只能看到“至少一个月”这个分段的强度。

5. 相关性与聚类
```{r corrplot, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# 相关性矩阵
num_df <- orders %>% select_if(is.numeric)
corr_mat <- cor(num_df, use="pairwise.complete.obs") # 计算每一对数值列的皮尔逊相关系数，自动跳过缺失值。结果是一个 5×5 的对称矩阵，元素值在 –1 到 +1 之间
corrplot::corrplot(corr_mat, method="ellipse") # 用椭圆的形状和颜色深浅来可视化相关系数： 深蓝＆细长 表示强正相关（接近 +1）；深红＆扁宽 表示强负相关（接近 –1）；浅灰/扁圆 表示无或弱相关（接近 0）
```
order_number 与 user_id：正相关很高（因为每个用户的订单序号从 1 开始自增），这完全是数据结构决定的。
order_number 与 days_since_prior_order：轻微负相关，说明用户下单次数越多，平均每次下单间隔往往越短（忠诚用户复购更频繁）。
其他组合同样反映了“用户活跃度”（高 order_number ↔ 低 days_since_prior_order）等直观关系。


```{r K-Means Clustering, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# 用户聚类
cluster_df <- rfm %>% select(recency, frequency) %>% scale()
set.seed(42)
km <- kmeans(cluster_df, centers=4)
rfm$cluster <- factor(km$cluster)

rfm %>% 
  # 去掉可能的 NA
  filter(!is.na(recency) & !is.na(frequency) & !is.na(cluster)) %>% 
  ggplot(aes(x = frequency, y = recency, color = cluster)) +
    geom_jitter(alpha = 0.4, width = 0.2, height = 0.2, size = 1.5) +
    scale_y_reverse() +   # recency 越小越近期，倒转 Y 轴更直观
    scale_x_continuous(labels = scales::comma) +
    labs(
      title    = "R–F 用户聚类分布",
      subtitle = "每个点代表一个用户，颜色表示 K-means 分群",
      x        = "Frequency (总订单数)",
      y        = "Recency (距今天数，倒序)",
      color    = "Cluster"
    ) +
    theme_minimal() +
    theme(
      legend.position = "right",
      plot.subtitle   = element_text(size = 10)
    )
```



之前的RFM是用分位数分箱 (ntile) 的方法绘制 R 和 F 网格
按新近度对用户进行排序，并为每个指标将其划分为 **5 个大小相等的桶**（分位数），然后绘制 R_score 与 F_score 的关系图。
**优点：**非常透明且易于解释（“此单元格是最近前 20% 的数据，也是最常出现的前 20% 的数据”）。
**缺点：** 1.即使数据分布不平衡，也会在第 20、40、60、80 个百分位数处施加**严格的截断**。 2.不考虑联合分布的“形状”；单独处理每个轴。

K 均值聚类 — “基于距离的细分”
将两个指标标准化（均值 0，标准差 1），然后运行 ​​K 均值聚类将用户划分为 **4 个聚类**，以最小化二维 R-F 空间中的聚类内方差。
**输出：** 四个组，每个组都有各自的质心（例如“高频/高频”、“低频/中频”等）。
**优点：** 1.聚类基于**实际数据密度**和自然分组，而非任意百分位数。 2.可以根据业务需求选择任意数量的聚类。
**缺点：** 1.如果利益相关者期望“前 20% = VIP”的逻辑，则更难解释。 2.必须事后解释每个聚类的含义（例如，聚类 1 =“新手”，聚类 2 =“潜在高消费用户”）。

如果认为客户构成是**自然群体**（例如，与 20% 的削减不一致的中频“最佳点”），并且希望数据能够驱动细分定义，那么**K 均值**会是更好的选择。

散点图“太”线性了，因为**新近度**计算方式是：recency = as.numeric(today() - max(order_number)) 实际上，它的作用是 recency_i = today() - (用户 i 的最高 order_number) 所以，如果用户 i 下了 50 个订单，就是在执行 `today() - 50`——订单数量和“新近度”之间完美的一一对应关系，因此是直线对角线

修复“最近成交量”→使用自上次订单以来的实际天数

```{r K-Means Clustering2, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
library(forcats)
# 1. Build pseudo-calendar orders_fake first (if not already in env)
base_date <- as.Date("2024-01-01")

orders_fake <- orders %>% # # 为每个订单重建一个伪日历日期
  arrange(user_id, order_number) %>%
  group_by(user_id) %>%
  mutate(
    days       = replace_na(days_since_prior_order, 0),
    cum_days   = cumsum(days),
    order_date = base_date + cum_days
  ) %>%
  ungroup()

# 2. Compute RFM properly from orders_fake
rfm2 <- orders_fake %>%
  group_by(user_id) %>%
  summarise(
    recency   = as.numeric(today() - max(order_date)),  # 自上次实际 order_date 以来的天数
    frequency = n(),                                    # 订单总数
    .groups   = "drop"
  )

# 3. K-means clustering
cluster_df <- rfm2 %>% select(recency, frequency) %>% scale()
set.seed(42)
km <- kmeans(cluster_df, centers = 4)
rfm2$cluster <- factor(km$cluster)

# 4. Plot the clusters in RF space
rfm2 %>%
  ggplot(aes(x = frequency, y = recency, color = cluster)) +
    geom_jitter(alpha = 0.4, size = 1.5, width = 0.2, height = 0.2) +
    scale_y_reverse() +  
    scale_x_continuous(labels = scales::comma) +
    labs(
      title    = "R–F 用户聚类分布（Recency = days since last order）",
      x        = "Frequency (总订单数)",
      y        = "Recency (天, 倒序)",
      color    = "Cluster"
    ) +
    theme_minimal()

```

1. **`order_date` 是一个真正的日历日期**（尽管是伪日历），通过累计真实的 `days_since_prior_order` 偏移量。
2. **`recency = today() - max(order_date)`** 现在衡量的是自用户上次下单以来的实际天数。
3. 由于 **频率**（订单总数）和 **新近度**（自上次下单以来的天数）现在是 **两个独立的真实世界信号**，因此它们之间不存在强制的线性关系。


聚类颜色 新近度 频率 业务解读
1 🔴 红色 高 低 风险/休眠
订单总数很少，并且很长时间没有购买。适合积极地进行挽回营销活动。
2 🟢 绿色 低 高 冠军
下单频率很高，并且刚刚购买——您最忠诚、价值最高的客户。重点关注 VIP 权益。
3 🔵 蓝绿色 中 低-中 潜力
下单频率适中，并且距离上次下单时间不长。可以通过有针对性的优惠活动将其转化为常客。
4 🟣 紫色 低-中 中-高 忠诚但老化
下单频率不错，但很久没有下单了。他们正在流失——提醒邮件或新的激励措施可以重新激活他们。
