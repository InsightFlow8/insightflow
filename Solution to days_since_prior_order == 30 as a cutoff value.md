# Solution to `days_since_prior_order == 30` as a cutoff value

## 问题概述

- **缺少真实时间戳**：订单仅记录
  - 星期几（`order_dow`），
  - 一天中的小时（`order_hour_of_day`），
  - 以及上限为 30 天的订单间隔（`days_since_prior_order`）
- **删失效应**：任何 30 天或以上的间隔都会被记录为恰好 30 天
  - 直接使用 `order_date = today() – days_since_prior_order` 会把所有 `days_since_prior_order == 30` 的记录堆到同一日，导致可视化中“30 天”截止值处出现大幅峰值
  - 从此字段派生的特征（例如 `recency_days`）严重低估了较长的时间间隔
- 可视化而做的“伪日历”反而引入了集中尖峰或周期性失真

---

## 7月20号之前的方案 （废案）

### 构造相对时间指标（Relative‐Time）

- **累积天数（cum_days）**
  对每位用户，按照订单序号（`order_number`）从小到大排序，对填补后的天数做累积和：

  > 第 1 单：0；第 2 单：两单间隔；第 3 单：两单间隔之和；……
  > 这样得到的 `cum_days` 就是“第 k 单距离首单过去了多少天”

- **新近度（Recency）**
  用分析截止日（或“今天”）减去最后一笔伪日历日期，得到真正的天数差；也可直接用用户的最大 `cum_days`，代表从首单到最近一次下单共经历了多少天

- **频率（Frequency）**
  直接取每位用户的最大 `order_number`，即该用户总下单次数

------

### 重建伪日历日期（Pseudo‐Calendar）

因为原始表里并没有真实的日历日期，需要基于用户间隔数据“还原”一个连续可视的时间轴：

1. **选定锚点（anchor）**
   设定一个“最后一次下单日”（`cutoff_date`），可以是分析当天（`today()`），也可以是一个固定的起始日期（例如 2024-01-01），此时所有用户在该日期都有一笔“最后一单”
2. **计算用户级锚点**
   对于每个用户，用 `cutoff_date - max(cum_days)` 作为其“第 1 单”对应的伪日历起点
3. **映射绝对日期**
   再把每条订单的 `cum_days` 加到该起点上，得到一个假想的 `order_date`，如此就把间隔数据分散到一个连续的日历范围内

------

### 消除“最后一单”尖峰的抖动策略（Jitter）

由于所有用户的最后一单都对齐到同一天，直接画伪日历会在 `cutoff_date` 处出现一个巨大尖峰，尝试了几种抖动方式来打散它：

1. **Uniform（均匀抖动）**
   让每个用户的锚点在 `[cutoff_date – D, cutoff_date]` 范围内随机平移一天到 D 天
2. **Truncated Normal（截断正态抖动）**
   用正态分布生成偏移值，并截断为 ≥ 1 天，大多数偏移靠近 0，少数偏移更远
3. **Weekly‐Aligned（整周对齐抖动）**
   先按 7 天为步长随机移动若干周（`7×k` 天），再加一个小范围天数抖动，既保留了“同一星期几”的周期波动，又分散了“最后一单”的集中

通过排除零抖动（范围从 1 开始）、扩大 D 值或对齐整周，既能去掉单日峰值，又能在伪日历视图里保留或恢复 7 天周期的波动

------

### 依然存在的问题

1. **伪日历重建的副作用**
    通过“锚点 + 累积偏移”方法，所有用户的“最后一单”仍会对齐到一个或一个狭窄区间，除非人为抖动 而抖动又会：
   - 消除尖峰，但扭曲真实节假日、促销日等时间特征
   - 难以兼顾“保留 7 天周期”与“平摊最后一单”两个目标，迫使在平坦化截止峰值和保留真实周期性之间做出权衡/选择 
2. **相对与绝对时间的错位**
    相对时间（`cum_days`）天然体现了复购衰减与 7 天周期，无需伪日历；但绝对日历视图对接业务事件（节假日、促销）有价值，两者各有利弊

---

## 7月20号之后的改进方案：基于多因素自回归 (MNAR) 的多重插补

结合 Rubin’s Rules 以及 Multiple Imputation，通过从临时抖动转向原则性的MNAR插补方法，纠正了截断区间的统计失真，使特征工程与真实用户行为保持一致，并为后续的机器学习流程提供透明、可量化的不确定性

---

### 1. 明确“缺失机制”（Missingness Mechanism）

在已有方案中，把首单 NA → 0，和 把 `days_since_prior_order == 30` 当作截断值，但从 Rubin 的分类来看，这两种缺失是截然不同的：

1. **首单 NA** 属于 **MCAR（Missing Completely at Random）**——首单数据本身就不存在，直接填 0 合理，不会引入系统性偏差 
2. **30 天截断** 是一种 **MNAR（Missing Not at Random）/Censored** 情况——任何超过 30 天的真实间隔都被硬性折叠到 30，且这种折叠与真实间隔（缺失值）本身有关 

**优化**：先用可视化和统计（如 VIM::aggr、mice::md.pattern）评估“截断”记录与其他字段（频率、最后一单周几/时段）之间的关联，判断是否存在 MNAR 风险

------

## 2. 对“截断值”做多重插补（Multiple Imputation）

Rubin 的多重插补（MI）流程有 **三步**：插补→分析→合并 该MI框架可以纠正偏差，保留合理的长间隔，并量化插补不确定性

1. **Imputation（插补）**
   - **模型选择**：针对 `days_since_prior_order` 超过 30 的那些“截断”值，构建预测模型候选算法包括：
     - **Predictive Mean Matching (PMM)**：生成与观测数据分布匹配的真实间隔值，避免插补出不合理的“大数值” 
     - **Stochastic Regression Imputation**：在线性模型预测值上加噪声，保留不确定性
   - **多变量共插补**：利用用户的 `order_dow`、`order_hour_of_day`、`frequency`、`recency_days`、篮子大小等作为协变量，通过 MICE（Iterative Chained Equations）一次性对所有含缺失的变量（这里主要是那些被截断的间隔）做插补 
   - **生成 m 个数据集**（m≈缺失比例百分比，例如 6%→m=6 [为了加速，我在EDA代码里设置为 5，后续可优化改成 6 甚至 7 或 10]），保留插补差异度
2. **Analysis（分析）**
   - 针对每个插补完的数据集，重复原来的“伪日历重建 + 特征工程”流程，得到 m 组 `order_date`、`cum_days`、RFM 等特征
3. **Pooling（合并）**
   - 按照 **Rubin’s Rules** 合并下游模型的参数和方差估计，既利用了所有用户信息，又正确反映了插补带来的不确定性 

------

## 3. 监控和质量控制

- **数据质量指标**：跟踪并提醒每日/每周以下比例：
  - 缺失（“NA”）`days_since_prior_order（首单）的间隔
  - 删失（“==30”）的间隔
- **自动化仪表盘**：使用Athena或QuickSight可视化趋势；设置自动通知的阈值（例如>15%）

------

## 4. 敏感性分析与报告

- **MNAR 情境模拟**：由于截断本身可能与用户行为（长周期流失）相关，建议在报告中做 **Best/Worst‐case** 和/或 **Delta‐adjustment** 敏感性测试（比如把所有截断值都向前／向后偏移固定量），评估模型输出对这些假设的稳健性

  - **1. 最佳与最坏情况**

    - 最佳：将所有删失值视为原始的 30 天

    - 最坏：将所有删失值视为更长的时间间隔（例如 60 天）

    - 比较 `recency_days` 的结果分布，以限制极端假设的影响

  - **2. Delta 调整**

    - 系统地将删失值调整 ±Δ（例如 -5、0、+5、+15 天）

    - 观察关键统计数据（平均值、中位数、IQR）的线性变化，以确认可预测的敏感性

      

- **透明披露**：

  - 在文档中说明“哪些字段为什么插补”、“使用了哪些协变量”、“m 取值多少”以及“MNAR 假设如何检验/放宽” 
  - **将敏感度结果**纳入模型评估和业务文档中，提供置信区间而非点估计

---

## 5、建议和后续步骤 结合“工程化”在 ETL／Pipeline 中实现

1. **参数化缺失策略**
   - 在生产流程中，**对所有删失应用MI**，并调整m和模型参数以提高效率和收敛性
   - 在 Glue Job、Airflow 或 SageMaker Processing 脚本中，把“插补算法”、“m 值”、“截断阈值”设为可配置参数，支持线上／线下快速切换与回溯
2. 在ETL工作流程中，**对缺失/删失比例进行持续监控**
   - 定期在 Athena／QuickSight 上监控 `days_since_prior_order` 的缺失 / 30 截断比例，若突增，自动触发检查流程（类似数据质量监控）
3. **探索分层插补**或特征离散化（例如，bucketed recency）以进一步减少特定用户群体的偏差