---
title: "Imba_data EDA"
author: "Zhenyu Zhang"
date: "2025-06-13"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(data.table)
library(purrr)
library(skimr)
library(tidyverse)
library(scales)
library(knitr)
library(lubridate)
library(DT)
library(ggplot2)
library(scales)
library(corrplot)
library(lubridate)
library(dplyr)
library(mice)
library(future)
library(VIM)
```


```{r load locol-data, warning=FALSE, message=FALSE}
orders                <- read_csv("orders.csv", show_col_types = FALSE)
products              <- read_csv("products.csv", show_col_types = FALSE)
aisles                <- read_csv("aisles.csv", show_col_types = FALSE)
departments           <- read_csv("departments.csv", show_col_types = FALSE)
order_products_prior  <- read_csv("order_products_prior.csv.gz", show_col_types = FALSE)
order_products_train  <- read_csv("order_products_train.csv.gz", show_col_types = FALSE)
```

---
1. raw data pre check
```{r pre_check, warning=FALSE, message=FALSE}
# consistency check Primary key uniqueness check
# prior
order_products_prior %>% 
  summarise(total = n(), 
            distinct_pairs = n_distinct(paste(order_id, product_id, sep = "_")))

# train
order_products_train %>% 
  summarise(total = n(), 
            distinct_pairs = n_distinct(paste(order_id, product_id, sep = "_")))
```
```{r pre_check_2, warning=FALSE, message=FALSE}
# Foreign key integrity check
# prior
order_products_prior %>% anti_join(orders,    by = "order_id")   %>% count()   # 应为 0
order_products_prior %>% anti_join(products,  by = "product_id") %>% count()   # 应为 0

# train
order_products_train %>% anti_join(orders,    by = "order_id")   %>% count()   # 应为 0
order_products_train %>% anti_join(products,  by = "product_id") %>% count()   # 应为 0
```


```{r pre_check_3, warning=FALSE, message=FALSE}
# Put all tables into a list
tables <- list(
  orders                 = orders,
  order_products_prior   = order_products_prior,
  order_products_train   = order_products_train,
  products               = products,
  aisles                 = aisles,
  departments            = departments
)

# 1. Total number of missing values in each table
map_int(tables, ~ sum(is.na(.))) %>% 
  enframe(name = "table", value = "n_missing") %>% 
  knitr::kable(caption = "Total number of missing values")

# 2. Missing rate by column in each table
map(tables, ~ map_dbl(., ~ mean(is.na(.)))) %>% 
  map_df(~ .x, .id = "table") %>% 
  pivot_longer(-table, names_to = "column", values_to = "missing_rate") %>% 
  filter(missing_rate > 0) %>% 
  arrange(desc(missing_rate)) %>% 
  knitr::kable(
    caption = "Missing rate by column in each table (only missing columns are listed)",
    digits = 3
  )

```

```{r pre_check_4, warning=FALSE, message=FALSE}
# 3. Calculate the number and rate of missing data for each table and column
missing_detail <- map_df(
  tables,
  ~ tibble(
      column       = names(.x),
      n_missing    = colSums(is.na(.x)),
      missing_rate = colMeans(is.na(.x))
    ),
  .id = "table"
) %>%
  filter(n_missing > 0) %>%
  arrange(table, desc(missing_rate))

missing_detail %>%
  knitr::kable(
    caption = "Detailed Missing Data by Table and Column",
    digits = c(0, 0, 3),
    col.names = c("Table", "Column", "Missing Count", "Missing Rate")
  )

```

```{r pre_check_5, warning=FALSE, message=FALSE}
orders_cleaned <- orders %>% 
  drop_na(days_since_prior_order)
```




```{r EDA for order table, warning=FALSE, message=FALSE}
# Display the first few rows of the tables
# head(orders)

# Use `spec()` and/or `skimr::skim()` to retrieve the full column specification for this data
spec(orders)
skimr::skim(orders)
spec(products)
spec(aisles)
spec(departments)
spec(order_products_prior)
spec(order_products_train)

```
---

2. Single table EDA（Explore Data Analysis）

2.0 准备工作
type conversion [把离散的数字列变成因子或整数，便于后续绘图与汇总]
```{r type conversion, warning=FALSE, message=FALSE}
orders <- orders %>% 
  mutate(
    order_dow          = factor(order_dow, levels = 0:6,
                                labels = c("Sun","Mon","Tue","Wed","Thu","Fri","Sat")),
    order_hour_of_day  = as.integer(order_hour_of_day),
    eval_set           = factor(eval_set, levels = c("prior","train","test"))
  )


orders_cleaned <- orders_cleaned %>% 
  mutate(
    order_dow          = factor(order_dow, levels = 0:6,
                                labels = c("Sun","Mon","Tue","Wed","Thu","Fri","Sat")),
    order_hour_of_day  = as.integer(order_hour_of_day),
    eval_set           = factor(eval_set, levels = c("prior","train","test"))
  )


order_products_prior <- order_products_prior %>% mutate_all(~ ifelse(is.na(.), ., .))
order_products_train <- order_products_train

products   <- products   %>% mutate(across(c(product_id, aisle_id, department_id), as.integer))
aisles     <- aisles     %>% mutate(aisle_id = as.integer(aisle_id))
departments<- departments%>% mutate(department_id = as.integer(department_id))

```


```{r orders, warning=FALSE, message=FALSE}
orders %>% 
  summarise(
    total_orders = n(),
    total_users  = n_distinct(user_id)
  )


```


2.1 订单 (orders)
按星期 & 小时分布
```{r orders distribution, warning=FALSE, message=FALSE}
# Order distribution by day of the week
orders_cleaned %>% 
  count(order_dow) %>% 
  ggplot(aes(order_dow, n)) +
    geom_col() +
    labs(
      title = "Order distribution by day of the week",
      x = NULL, y = "Order Number"
    ) +
    theme_minimal()

# Order distribution by hour
orders_cleaned %>% 
  count(order_hour_of_day) %>% 
  ggplot(aes(order_hour_of_day, n)) +
    geom_col() +
    scale_x_continuous(breaks = seq(0,23,by=2)) +
    labs(
      title = "Order distribution by hour",
      x = "Hour", y = "Order Number"
    ) +
    theme_minimal()

```

复购周期分布
```{r repurchase cycle distribution, warning=FALSE, message=FALSE}
orders_cleaned %>% 
  filter(!is.na(days_since_prior_order)) %>% 
  ggplot(aes(days_since_prior_order)) +
    geom_histogram(binwidth = 1) +
    scale_x_continuous(limits = c(0, 30)) +    # 截取前两个月更清晰
    labs(
      title = "Distribution of days between repurchases",
      x = "Days since last order", y = "Frequency"
    ) +
    theme_minimal()

```

用户下单次数分布
```{r distribution of order times, warning=FALSE, message=FALSE}
orders_cleaned %>% 
  count(user_id, name = "order_count") %>% 
  ggplot(aes(order_count)) +
    geom_histogram(
      breaks = seq(0, 100, by = 10),
      closed = "right",
      color  = "white"
    ) +
    # linear X axis with 10-unit ticks
    scale_x_continuous(
      breaks = seq(0, 100, by = 10),
      labels = comma_format()
    ) +
    # log10 on Y axis
    scale_y_continuous(
      trans  = "log10",
      labels = comma_format()
    ) +    labs(
      title = "Distribution of Orders Per User",
      x     = "Total Orders per User",
      y     = "Number of Users (log scale)"
    ) +
    theme_minimal()


```


2.2 订单–产品 (order_products)
```{r order_products1, warning=FALSE, message=FALSE}
# 合并示例
order_products <- bind_rows(
  order_products_prior  %>% mutate(src = "prior"),
  order_products_train  %>% mutate(src = "train")
)

order_products %>% 
  count(order_id, name="items_per_order") %>% 
  ggplot(aes(items_per_order)) +
    # bins of size 10: [0–10),[10–20)… up to 100
    geom_histogram(
      breaks = seq(0,100,by=10), 
      closed = "right", 
      color  = "white"
    ) +
    scale_x_continuous(
      breaks = seq(0,100,by=10),
      labels = seq(0,100,by=10)
    ) +
    scale_y_continuous(
      trans  = "log10",
      labels = scales::comma_format()
    ) +
    labs(
      title = "每笔订单的商品数量分布",
      x     = "商品数量",
      y     = "订单数 (log scale)"
    ) +
    theme_minimal()

```

```{r order_products2, warning=FALSE, message=FALSE}
# how often items land in each cart position
order_products %>% 
  count(add_to_cart_order) %>% 
  filter(add_to_cart_order <= 20) %>%   # 限制到前20个位置
  ggplot(aes(add_to_cart_order, n)) +
    geom_col() +
    labs(
      title = "加入购物车顺序分布（前20位）",
      x     = "加入顺序",
      y     = "频次"
    ) +
    theme_minimal()

```


```{r order_products3, warning=FALSE, message=FALSE}
order_products %>% 
  group_by(product_id) %>% 
  summarise(
    avg_pos = mean(add_to_cart_order),
    count   = n(),
    .groups = "drop"
  ) %>% 
  filter(count >= 1000) %>%    # 只看销量够大的产品
  slice_min(avg_pos, n = 10) %>% 
  left_join(products, by="product_id") %>% 
  ggplot(aes(fct_reorder(product_name, avg_pos), avg_pos)) +
    geom_col() +
    coord_flip() +
    labs(
      title = "平均加入购物车顺序最靠前的10种产品",
      x     = NULL,
      y     = "平均加入顺序"
    ) +
    theme_minimal()

```






```{r order_products4, warning=FALSE, message=FALSE}
# overall spread of reorder_rate across products
order_products %>% 
  group_by(product_id) %>% 
  summarise(
    reorder_rate = mean(reordered),
    sales        = n(),
    .groups      = "drop"
  ) %>% 
  ggplot(aes(reorder_rate)) +
    geom_histogram(bins = 50) +
    labs(
      title = "产品层面复购率分布",
      x     = "复购率",
      y     = "产品数"
    ) +
    theme_minimal()

# Top-10 复购率最高产品
order_products %>% 
  group_by(product_id) %>% 
  summarise(
    reorder_rate = mean(reordered),
    sales        = n(),
    .groups      = "drop"
  ) %>% 
  filter(sales >= 1000) %>% 
  slice_max(reorder_rate, n = 10) %>% 
  left_join(products, by="product_id") %>% 
  ggplot(aes(fct_reorder(product_name, reorder_rate), reorder_rate)) +
    geom_col() +
    coord_flip() +
    labs(
      title = "复购率最高的 10 种产品",
      x     = NULL,
      y     = "复购率"
    ) +
    theme_minimal()


# Prior vs Train 全局复购率
order_products %>% 
  group_by(src) %>% 
  summarise(
    reorder_rate = mean(reordered),
    .groups      = "drop"
  ) %>% 
  ggplot(aes(src, reorder_rate)) +
    geom_col() +
    labs(
      title = "Prior vs Train 全局复购率对比",
      x     = NULL,
      y     = "复购率"
    ) +
    theme_minimal()


```


2.3 产品维度
各部门 & 货架产品数
```{r order_products, warning=FALSE, message=FALSE}
products %>% 
  count(department_id, name = "n") %>% 
  left_join(departments, by="department_id") %>% 
  ggplot(aes(fct_reorder(department, n), n)) +
    geom_col() +
    coord_flip() +
    labs(title="各部门产品数", x=NULL, y="产品数") +
    theme_minimal()

products %>% 
  count(aisle_id, name="n") %>% 
  left_join(aisles, by="aisle_id") %>% 
  slice_max(n, n=10) %>% 
  ggplot(aes(fct_reorder(aisle, n), n)) +
    geom_col() +
    coord_flip() +
    labs(title="Top10 货架的产品数", x=NULL, y="产品数") +
    theme_minimal()


```
```{r define-prod-with-aisle, message=FALSE}
prod_with_aisle <- products %>% 
  left_join(aisles, by = "aisle_id")
```

```{r order_products_missing_aisle, warning=FALSE, message=FALSE}
# 1. 计算 missing‐aisle ratio
missing_stats <- products %>%
  # bring in the aisle names
  left_join(aisles, by = "aisle_id") %>%
  # now we can filter on the aisle column
  filter(aisle == "missing") %>%
  summarise(
    Missing_Count = n(),
    Total_Products = nrow(products),
    Missing_Pct = percent(Missing_Count / Total_Products)
  )

missing_stats %>% 
  kable(caption = "Products with aisle == 'missing'")

# 2. Filter out "missing" and plot Top 10
prod_with_aisle %>%
  filter(aisle != "missing") %>%        # drop the missing bucket
  count(aisle, name = "n") %>%          # count products by aisle
  slice_max(n, n = 10) %>%              # select top 10 by count
  ggplot(aes(fct_reorder(aisle, n), n)) +
    geom_col() +
    coord_flip() +
    labs(
      title = "Top 10 货架的产品数（排除 'missing'）",
      x     = NULL,
      y     = "产品数"
    ) +
    theme_minimal()
```


复购率最高 TopN 产品
```{r order_products_TopN, warning=FALSE, message=FALSE}
order_products %>% 
  group_by(product_id) %>% 
  summarise(
    sales = n(),
    reorder_rate = mean(reordered)
  ) %>% 
  filter(sales > 1000) %>%             # 过滤销量太小的噪声
  slice_max(reorder_rate, n=10) %>%    # 复购率 Top10
  left_join(products, by="product_id") %>% 
  ggplot(aes(fct_reorder(product_name, reorder_rate), reorder_rate)) +
    geom_col() +
    coord_flip() +
    labs(
      title="复购率最高的 10 种产品",
      x=NULL, y="复购率"
    ) +
    theme_minimal()

```


3. 多表联合分析
3.1 热门部门 & 通道

```{r Popular Departments & Channels, warning=FALSE, message=FALSE}
order_products %>% 
  count(product_id, name="sales") %>% 
  left_join(products,    by="product_id") %>% 
  left_join(departments,by="department_id") %>% 
  group_by(department) %>% 
  summarise(
    total_sales   = sum(sales),
    avg_reorder   = mean(order_products$reordered[order_products$product_id %in% product_id])
  ) %>% 
  arrange(desc(total_sales)) %>% 
  slice_head(n=10)


```

3.2 用户购买行为分群（RFM）
```{r RFM, warning=FALSE, message=FALSE}
# RFM 示例
rfm <- orders %>% 
  group_by(user_id) %>% 
  summarise(
    recency   = as.numeric(today() - max(order_number)),  # 若有具体日期字段请替换
    frequency = n(),
    monetary  = NA_real_                             # 无金额字段可留空或用 order_number 估算
  )

# 简易分群
rfm %>% 
  mutate(
    R_score = ntile(-recency, 5),
    F_score = ntile(frequency, 5)
  ) %>% 
  ggplot(aes(F_score, R_score)) +
    geom_jitter(alpha=0.3) +
    labs(
      title="用户 RFM 分布 (简易版)",
      x="Frequency Score", y="Recency Score"
    ) +
    theme_minimal()


```

RFM 基础知识
Recency (R)新近度 (R)：距离客户上次下单的时间。

Frequency (F) 频率 (F)：他们总共下单了多少单。

(Monetary (M) is left out here because we don’t have $-value fields.)

综合起来，RFM 用于识别以下群体：

R-score	      F-score	          Who these are
High        	High	            Champions (最佳!)
High        	Low	              New or About-to-Sleep 新客户或即将入驻（有时高新近度仅指首次下单）
Low	          High            	Loyal but Inactive 忠诚但不活跃（过去经常购买，但最近没有）
Low	          Low	              At Risk or Lost 处于风险或可能会流失





4. 时序与趋势
```{r time, warning=FALSE, message=FALSE}
# 日级订单量
orders %>% 
  mutate(order_date = today() - days_since_prior_order) %>%  # 需真实日期列替换
  count(order_date) %>% 
  ggplot(aes(order_date, n)) +
    geom_line() +
    labs(title="日级订单量趋势", x=NULL, y="订单数") +
    theme_minimal()


```
5 月 26 日左侧的峰值是由于合成 `order_date` 的方式造成的：
`orders.csv` 中**实际上没有真正的日历日期**——只有
* `order_dow` (0-6)
* `order_hour_of_day`
* `days_since_prior_order`（上限为 30；不适用于首单）
通过执行
mutate(order_date = today() - days_since_prior_order)
所有 `days_since_prior_order == 30` 的订单（即所有超过 30 天的订单）映射到**一个单一日期**（今天减 30）。这会将数百万行数据集中到该日期，因此会出现巨大的峰值。

如何修复
1. 真实的订单时间戳 - 没有。。。
2. 使用起始日期锚点进行近似
    为每个用户的第一个订单选择一个任意的“锚点”日期，然后累计添加 `days_since_prior_order` 偏移量，以重建伪日历
    a. 先把每个用户的 NA（首单）当作 0 天间隔。
    b. 对每个用户按 order_number 累加这些天数，得到 cum_days。
    c. 假设「最后一次下单」对应今天（或一个指定的固定日期），则锚点 anchor_date = today() − max(cum_days)
    d. 最终日期 = anchor_date + cum_days。
    
```{r Pseudo-calendar, warning=FALSE, message=FALSE}
# 以今天为“最后一次下单日”基准；也可改成 as.Date("2025-06-23")
base_date <- today()

orders_with_dates <- orders %>%
  arrange(user_id, order_number) %>% 
  group_by(user_id) %>% 
  mutate(
    # 将 NA（首单）视为 0 天
    days = replace_na(days_since_prior_order, 0),
    # 累积天数
    cum_days = cumsum(days),
    # 计算每个用户的锚点日期
    anchor_date = base_date - max(cum_days),
    # 重建伪订单日期
    order_date = anchor_date + cum_days
  ) %>% 
  ungroup() %>% 
  select(-days, -cum_days, -anchor_date)

# 看一下示例
orders_with_dates %>% 
  filter(user_id %in% c(1,2,3)) %>% 
  select(user_id, order_number, days_since_prior_order, order_date) %>% 
  head(10) %>% 
  knitr::kable()

```

```{r Trend Chart, warning=FALSE, message=FALSE}
orders_with_dates %>% 
  count(order_date) %>% 
  ggplot(aes(order_date, n)) +
    geom_line() +
    labs(
      title = "伪日历下的日级订单量趋势",
      x = NULL, y = "订单数"
    ) +
    theme_minimal()
```


锚定“上次订单 = 今天”会导致不正常的峰值 （不可行）

两种更简单的替代方案
A) 绘制 相对时间轴
```{r relative timeline, warning=FALSE, message=FALSE}
orders_rel <- orders %>%
  arrange(user_id, order_number) %>%
  group_by(user_id) %>%
  mutate(
    days     = replace_na(days_since_prior_order, 0),
    cum_days = cumsum(days)
  ) %>%
  ungroup()

# Only re-orders (drop cum_days == 0)
orders_rel %>%
  filter(cum_days > 0) %>%
  count(cum_days) %>%
  ggplot(aes(cum_days, n)) +
    geom_line() +
    labs(
      title = "Trend by Days Since First Order (Re-orders Only)",
      x     = "Days Since First Order",
      y     = "Re-order Count"
    ) +
    theme_minimal()

```



B) 锚定到**中点**或人为设定的“开始日期”
```{r artificial start date, warning=FALSE, message=FALSE}
base_date <- as.Date("2024-01-01")  # fixed start

orders_fake <- orders %>%
  arrange(user_id, order_number) %>%
  group_by(user_id) %>%
  mutate(
    days       = replace_na(days_since_prior_order, 0),
    cum_days   = cumsum(days),
    order_date = base_date + cum_days
  ) %>%
  ungroup()

# Only re-orders (drop the zero-day, i.e. first orders)
orders_fake %>%
  filter(order_date > base_date) %>%
  count(order_date) %>%
  ggplot(aes(order_date, n)) +
    geom_line() +
    labs(
      title = "Pseudo-calendar Trend (Re-orders Only, Jan 1 2024 start)",
      x     = "Order Date",
      y     = "Re-order Count"
    ) +
    theme_minimal()

```

这两张图展示的是每个用户在首次下单以后各个“天”上的复购次数，只统计了cum_days > 0（也就是第二次及以后的订单），从而剔除了那条“所有人首单集中在 0 天” 的巨大峰值

1. 30 天处的尖峰
    由于原始字段 days_since_prior_order 被 截断在 0–30 天，所有超过 30 天的实际复购都被当作 30 天来记录，所以在“30 天”位置出现一个异常大的峰值。但是当一个用户继续下第 4、5、6… 单时，会把一次次的“30 天”累加进。因此，cum_days 会很自然地跑到 60、90、120… 这些数值——对应用户在第 2、3、4… 次复购时，每次都“至少 30 天”后才买
2. 7 天为周期的波动
    可以看到除了 30 天主峰以外，图上还有许多小峰，大约间隔 7 天出现一次——这反映了用户有 每周购物 的行为习惯。
3. 随时间的衰减趋势
    随着自首次下单以来的天数增加，复购次数在整体上呈 指数型衰减，说明大多数用户会随着时间推移逐渐减少下单频率，只有少数“高忠诚度”用户会持续复购。
4. 前几天的上升
    在第 1–10 天内的曲线从零点上来会有一个上升期，说明很多用户喜欢在首次尝试后不久就进行第二次购买。

价值与应用

  周期性促销：可以针对“每周复购”的人群设计周期性营销（例如 7 日回购提醒）。
  生命周期管理：根据复购衰减曲线，识别“流失高风险期”（比如第 20–30 天）并推送激励。
  数据校准：30 天截断峰提醒我们，用此字段做时间分析时要小心截断偏差；如果需要，更细粒度的天数数据会更准确。
  数据限制提示：峰值都是“≥30 天”打包，因此无法区分真实是 31 天还是 90 天，只能看到“至少一个月”这个分段的强度。
  

添加抖动（jitter）也许可以在可视化和简单分析中缓解 “所有用户最后一单都聚集在同一天” 所造成的人为尖峰。 核心思路是在每个用户的锚点（anchor_date）上附加一个小范围的随机偏移，使“最后一次下单”被沿时间轴均匀或近似地打散。抖动的形式（随机分布）有均匀分布 (Uniform) & 截断正态分布 (Truncated Normal)
优缺点权衡：
优点	                          缺点
• 消除伪尖峰，图表更自然	      • 引入人工噪声，绝对日期失真
• 保留用户相对顺序和间隔模式  	• 如果分析依赖真实节假日/事件日期，抖动后无法捕捉
• 简单易行，不需要额外数据	    • 抖动范围或分布设置不当会过度扭曲
```{r Overlay Jitter 1, warning=FALSE, message=FALSE}
# 均匀分布 - 每个用户的最后一次下单会均匀分散在 [cutoff_date - D, cutoff_date] 这段区间内
D <- 7  # D 是期望打散的天数范围/均匀抖动范围 ，比如这是设置为一周
# 截断正态分布 - 大多数用户的最后订单靠近 cutoff_date，而只有少数用户分布较远
σ <- 3  # 标准差，决定分散程度

cutoff_date <- today()

# 1. Uniform Jitter
orders_uni <- orders_rel %>%
  group_by(user_id) %>%
  mutate(
    jitter_u   = round(runif(1, min = 0, max = D)),
    anchor_u   = cutoff_date - max(cum_days) - jitter_u,
    order_date = anchor_u + cum_days
  ) %>%
  ungroup()

# 2. Truncated Normal Jitter
orders_trunc <- orders_rel %>%
  group_by(user_id) %>%
  mutate(
    jitter_t   = pmax(0, round(rnorm(1, mean = 0, sd = σ))),
    anchor_t   = cutoff_date - max(cum_days) - jitter_t,
    order_date = anchor_t + cum_days
  ) %>%
  ungroup()

# 3. Compute counts by cum_days for each jitter type
df_uni_cal   <- orders_uni   %>% filter(cum_days>0) %>% count(order_date) %>% mutate(Jitter="Uniform")
df_trunc_cal <- orders_trunc %>% filter(cum_days>0) %>% count(order_date) %>% mutate(Jitter="Truncated Normal")
df_cal       <- bind_rows(df_uni_cal, df_trunc_cal)

# 4. Combine and plot
ggplot(df_cal, aes(x = order_date, y = n, color = Jitter, linetype = Jitter)) +
  geom_line(size = 0.5, alpha = 0.7) +
  labs(
    title = "Pseudo-calendar Trend with Uniform vs Truncated Normal Jitter",
    x     = "Order Date",
    y     = "Re-order Count"
  ) +
  theme_minimal()
```

Uniform 抖动 里，jitter_u = round(runif(1, 0, D)) 会有不少 jitter_u == 0，于是这些用户的最后一单就被映射到 order_date = cutoff_date - 0 = cutoff_date，形成一个巨量尖峰。

Truncated Normal 抖动 更明显：截断后 jitter_t 里绝大多数也是 0，只有少数>0，所以更集中过多点。

想同时保留周周期＋拉平“最后一单”尖峰，有两种改法：
1. 抖动钉到整周：
把 jitter_u 限制为“整周偏移”，例如
# 0–W 周随机偏移，每周保持同一天
W = 4  # 最多四周内
jitter_u = 7 * sample(0:W, 1)
这样所有订单都在同一星期几上（不打散周周期），但用户“最后一单”会被平移 0、7、14、21、28 天。

2. 只排除当天（不抖动 0）＋保留小范围抖动
# 保证 jitter_u ≥ 1 且 <7，这样只打散到其它六个工作日
jitter_u = sample(1:6, 1)
周周期依然能在图上若隐若现地看到（因为偏移都是 1~6 天），但“0 天”那根尖峰被移走了

```{r Overlay Jitter 2, warning=FALSE, message=FALSE}
# 使用整周抖动（最多 4 周）＋ D 内小抖动
W = 4      # 最多 4 周
d = 3      # 再加 0–3 天的小抖动
sigma = 1  # 小正态噪声（可选）

orders_weekly <- orders_rel %>%
  group_by(user_id) %>%
  mutate(
    # 整周偏移 + 小天数抖动
    jitter_u = 7 * sample(0:W,1) + sample(0:d,1),
    anchor_u   = cutoff_date - max(cum_days) - jitter_u,
    order_date = anchor_u + cum_days
  ) %>%
  ungroup()

orders_weekly %>%
  filter(cum_days>0) %>%
  count(order_date) %>%
  ggplot(aes(order_date,n)) +
    geom_line() +
    labs(
      title="Pseudo-calendar with Weekly-aligned Jitter",
      x="Order Date", y="Re-order Count"
    ) +
    theme_minimal()


```
---
问题分析
1. 缺失与截断概况
首先统计原始数据中两类关键情况：
  首单 NA
  “≥30 天”截断
```{r Missingness situration, warning=FALSE, message=FALSE}
orders %>% 
  summarise(
    total      = n(),
    na_first   = sum(is.na(days_since_prior_order)),
    trunc30    = sum(days_since_prior_order == 30, na.rm = TRUE),
    prop_na    = na_first   / total,
    prop_trunc = trunc30    / total
  )

```


2. 缺失机制判定
根据 Rubin 对缺失机制的分类：
  MCAR（完全随机缺失）：缺失与任何数据值都无关
  MAR（条件随机缺失）：缺失与观测到的其他变量有关
  MNAR（非随机缺失/Censoring）：缺失（或截断）与自身未观测值有关 

在已有方案中，我们只是把首单 NA → 0，把 `days_since_prior_order == 30` 当作截断值。但从 Rubin 的分类来看，这两种缺失是截然不同的：
  1. **首单 NA** 属于 **MCAR（Missing Completely at Random）**——首单数据/业务逻辑 本身就不存在，直接填 0 合理，不会引入系统性偏差
  2. **30 天截断** 是一种 **MNAR（Missing Not at Random）/受限观测（censoring）** 情况——任何超过 30 天的真实间隔都被硬性折叠到 30，且这种折叠与真实间隔（缺失值）本身有关 

3. 缺失模式可视化
使用 mice 和 VIM 包检查缺失与截断在多变量中的分布,评估“截断”记录与其他字段（频率、最后一单周几/时段）之间的关联，判断是否存在 MNAR 风险：

```{r Missingness Mechanism, warning=FALSE, message=FALSE}
library(mice)
library(VIM)

# 缺失模式表
md.pattern(orders[, c("days_since_prior_order", "order_number", 
                             "order_dow", "order_hour_of_day")])

# 整体缺失与截断可视化
aggr(orders[, c("days_since_prior_order", "order_number", 
                        "order_dow", "order_hour_of_day")],
     numbers = TRUE, prop = TRUE, sortVars = TRUE)

```
只有 days_since_prior_order 存在 NA 或截断，其他字段 (order_number、order_dow、order_hour_of_day) 完整无缺失，这说明我们不需要为其它列做额外插补，但 days_since_prior_order 的截断值得重点关注。


4. 构造相对时间（cum_days）与用户汇总表
用 orders 表先计算出每笔订单的累积天数 cum_days，并据此在后续步骤中替代对 order_date 的需求。

```{r cum_days, warning=FALSE, message=FALSE}
orders_rel <- orders %>%
  arrange(user_id, order_number) %>%
  group_by(user_id) %>%
  mutate(
    days0    = replace_na(days_since_prior_order, 0),
    cum_days = cumsum(days0)
  ) %>%
  ungroup()

```


5. MNAR 风险初步定量检验
思路：判断哪些 order 被截断（days_since_prior_order == 30）并打上截断标记，再按用户汇总，检验“截断比例”与用户“频率／新近度”是否相关

```{r MNAR risk test, warning=FALSE, message=FALSE}
# 5.1 用户级聚合
orders_user <- orders_rel %>%
  mutate(is_trunc = (days_since_prior_order == 30)) %>%
  group_by(user_id) %>%
  summarise(
    frequency    = max(order_number,        na.rm = TRUE),
    recency_days = max(cum_days,            na.rm = TRUE),  # 用 cum_days 替代 order_date
    prop_trunc   = mean(is_trunc,           na.rm = TRUE)
  ) %>%
  ungroup()

# 5.2 相关性检验
cor_freq <- cor.test(orders_user$frequency,    orders_user$prop_trunc)
cor_recv <- cor.test(orders_user$recency_days, orders_user$prop_trunc)

list(cor_with_frequency    = cor_freq,
     cor_with_recency_days = cor_recv)

```
为每个用户计算“截断比例” (prop_trunc)，即一位用户多少笔订单被标记为 30 天
与两大核心行为指标做皮尔逊相关检验：
  与频率（frequency）相关：r ≈ –0.455，p < 2.2e–16
  与新近度（recency_days）相关：r ≈ –0.264，p < 2.2e–16

这两条负相关表明：
  下单越多的用户，其“截断”记录反而越少；
  距首单时间越长的用户，被截断的比例也略低。

换言之，“截断”并非随机发生，而是与用户行为密切相关，属于 MNAR（Missing Not At Random）。


小结：
  首单 NA：可安全填 0。
  ≥ 30 天截断：与用户行为（频率、新近度）显著相关，不能简单当做真实 30 天或随机填充，需用多重插补等方法来尊重这一不确定性。

---
基于这一 MNAR 发现，对“截断值”做合理的多重插补（Multiple Imputation）确保对截断值的处理既不失偏差，又能量化不确定性

多重插补（MI）概念回顾，Rubin 的多重插补（MI）流程有 **三步**：插补→分析→合并 
1. **Imputation（插补）**：对每个缺失值生成 m 份插补（m≈缺失百分比）
   - **模型选择**：针对 `days_since_prior_order` 超过 30 的那些“截断”值，构建预测模型。候选算法包括：
     - **Predictive Mean Matching (PMM)**：生成与观测数据分布匹配的真实间隔值，避免插补出不合理的“大数值” 
     - **Stochastic Regression Imputation**：在线性模型预测值上加噪声，保留不确定性。
   - **多变量共插补**：利用用户的 `order_dow`、`order_hour_of_day`、`frequency`、`recency_days`、篮子大小等作为协变量，通过 MICE（Iterative Chained Equations）一次性对所有含缺失的变量（这里主要是那些被截断的间隔）做插补
   - **生成 m 个数据集**（m≈缺失比例百分比，例如 30%→m=30），保留插补差异度
2. **Analysis（分析）**：在每个插补数据集上分别计算所需统计量或模型
   - 针对每个插补完的数据集，重复原来的“伪日历重建 + 特征工程”流程，得到 m 组 `order_date`、`cum_days`、RFM 等特征
3. **Pooling（合并）**：利用 Rubin’s Rules 合并 m 个结果，计算总体估计值与总方差
   - 按照 **Rubin’s Rules** 合并下游模型的参数和方差估计，既利用了所有用户信息，又正确反映了插补带来的不确定性 

1. 数据准备
```{r Data preparation for MI, warning=FALSE, message=FALSE}
# 在原始 orders_rel 中，将所有 days_since_prior_order == 30 视为“缺失”，并保留一列标记
imp_data <- orders_rel %>%
  mutate(
    days_imp    = ifelse(days_since_prior_order == 30, NA, days_since_prior_order),
    is_trunc    = (days_since_prior_order == 30)
  ) %>%
  # 选择用于插补的变量：days_imp（待插补）、order_number、order_dow、order_hour_of_day、cum_days、is_trunc
  select(user_id, order_number, order_dow, order_hour_of_day, cum_days, is_trunc, days_imp)

```


2. 多重插补设置
  2.1 生成预测矩阵：确保 days_imp 由其他列预测，但不自我预测
  2.2 指定插补方法：对 days_imp 使用 PMM
  2.3 选择插补份数 𝑚：按“缺失比例”的经验法则，设 𝑚 ≈ 10（大约等于 10.8%）

**以下这段没优化过，太耗时，不跑**
```{r MI settings, warning=FALSE, message=FALSE}
# 2.1 构造 predictorMatrix
predM <- make.predictorMatrix(imp_data)
predM["days_imp", "days_imp"] <- 0
# 让 is_trunc 也作为协变量，帮助模型识别截断行为
predM["days_imp", "is_trunc"] <- 1

# 2.2 指定方法：只对 days_imp 用 PMM，其他变量不插补
meth <- make.method(imp_data)
meth[] <- ""             # 先清空所有
meth["days_imp"] <- "pmm"

# 2.3 执行多重插补
set.seed(2025)
imp <- mice(
  data           = imp_data,
  m              = 10,    # 生成 10 个插补数据集
  method         = meth,
  predictorMatrix= predM,
  maxit          = 20     # 迭代 20 次以确保收敛
)


```

2.4 MICE 在默认配置下会对我们标记为缺失（days_imp == NA，约 37 万条记录）的那一列：
  2.4.1.生成 𝑚=10 份（m = 10）完整数据集
  2.4.2.每份数据迭代 maxit = 20 次，每次都要对所有缺失值做一次 Predictive Mean Matching 插补
      所以总共要做 10×20=200轮“挖补全表”操作——数据量级是 37 万个缺失值 × 200 ≈ 7.4 × 10的7次方，次插补加上每次插补都要从数十万个观测中挑“最相近的”几个样本，会非常非常耗时。

2.5 加速／优化策略：
  2.5.1. 减少 m 和 maxit：
    m 插补份数不必太大，常用 5–7 即可
    maxit 一般 5–10 次就足够收敛
  2.5.2. 精简预测矩阵（predictorMatrix）：
    只用最重要的协变量来预测 days_imp，比如只保留 order_number、cum_days，去掉 order_dow、order_hour_of_day。
  2.5.3. 并行运算
    使用 future 与 mice 的并行接口，一次并行跑多个插补（mice 3.13+ 支持）
  2.5.4. 降低粒度
    如果后续只需要 “每个用户一行” 的特征（比如 recency_days、frequency），可以先 按用户聚合，只对聚合后的表做插补，缺失条数就大幅减少。
    
以下为 1-3 的加速／优化 **[还是太费时了，不跑]**
```{r MI_Optimized_1, warning=FALSE, message=FALSE}
# 并行设置：根据你机器的核数调整 workers
plan(multicore, workers = 4)

# ——— 1. 重构 imp_data ———
imp_data <- orders_rel %>%
  mutate(
    days_imp = ifelse(days_since_prior_order == 30, NA, days_since_prior_order),
    is_trunc = (days_since_prior_order == 30)
  ) %>%
  select(user_id, order_number, cum_days, is_trunc, days_imp)

# ——— 2. 构造更精简的 predictorMatrix ———
predM <- make.predictorMatrix(imp_data)
# 不让 days_imp 自己预测自己，也不要用 user_id
predM["days_imp", c("days_imp", "user_id")] <- 0
# 只保留这三列做预测：order_number, cum_days, is_trunc
keep <- c("order_number", "cum_days", "is_trunc")
predM["days_imp", setdiff(names(imp_data), c(keep, "days_imp"))] <- 0

# ——— 3. 指定插补方法 ———
meth <- make.method(imp_data)
meth[] <- ""              # 清空所有默认方法
meth["days_imp"] <- "pmm" # 仅用 PMM 插补 days_imp

# ——— 4. 执行多重插补 ———
set.seed(2025)
imp_opt <- mice(
  data            = imp_data,
  m               = 5,             # 插补份数降到 5
  method          = meth,
  predictorMatrix = predM,
  maxit           = 10,            # 迭代次数降到 10
  seed            = 2025,
  parallel        = "multicore"    # 并行模式
)

# 查看收敛诊断
plot(imp_opt, c("days_imp"))
```

以下为 降低粒度 的加速／优化 **[跑这段]**
```{r MI_Optimized_2, warning=FALSE, message=FALSE}
# 并行设置 — Parallel settings —
## Windows / RStudio (use multisession)
n_workers_windows <- parallel::detectCores(logical = FALSE) - 1
plan(multisession, workers = n_workers_windows)

## Linux / WSL / macOS (use multicore; uncomment if on non-Windows)
# n_workers_linux <- parallel::detectCores(logical = TRUE) - 1
# plan(multicore, workers = n_workers_linux)

# 1. 按 user 级聚合，注意 na.rm = TRUE 
# Aggregate to user level with na.rm = TRUE
user_imp <- orders_rel %>%
  group_by(user_id) %>%
  summarise(
    # 先用 na.rm = TRUE 求 max，再判断截断
    max_days    = max(days_since_prior_order, na.rm = TRUE),
    days_imp    = ifelse(max_days == 30, NA, max_days),
    frequency   = max(order_number,      na.rm = TRUE)
  ) %>%
  select(-max_days) %>%
  ungroup()

# 确认非 NA 的 days_imp 还有多少
#Confirm how many non‐NA days_imp remain
table(is.na(user_imp$days_imp))

# 2. 构造 predictorMatrix：days_imp ← frequency
pm_user <- matrix(
  c(0, 1,   # days_imp predicted by frequency
    0, 0),  # frequency 不插补
  ncol = 2, byrow = TRUE,
  dimnames = list(c("days_imp", "frequency"),
                  c("days_imp", "frequency"))
)

# 3. 指定方法 Specify methods: only pmm for days_imp
meth_user <- c("pmm", "")  # 只插补 days_imp

# 4. 执行 MI
set.seed(2025)
imp_user <- mice(
  data            = user_imp,
  m               = 5,
  method          = meth_user,
  predictorMatrix = pm_user,
  maxit           = 10,
  seed            = 2025,
  parallel        = "multicore"
)

# 5. 可选：检查收敛
plot(imp_user, "days_imp")
```
2.6 结果分析
  2.6.1. table(is.na(user_imp$days_imp)):
    总用户数：54197 + 152012 = 206 209。
    FALSE (54 197)：这部分用户在聚合后有真实的最大间隔（即他们 max(days_since_prior_order) < 30），不需要插补。
    TRUE (152 012)：这部分用户的最大间隔原本等于 30，被我们标记为 NA，需要由 MICE 去插补。

  2.6.2. MICE 插补日志（iter imp variable）:
    iter：当前第几次迭代（总共 maxit = 10）。
    imp：当前针对第几个插补链（总共 m = 5）在运行。
    variable：要插补的变量 (days_imp)。
    也就是说，MICE 会对每条链（chain）做一次“PMM 插补——更新缺失的 days_imp”，然后迭代多次（这里共 10 次）。

  2.6.3. 收敛诊断图（plot(imp_user, "days_imp")）
    (左：插补值的平均数；右：插补值的标准差)
    每条彩色线：表示一条“插补链”在各次迭代中，插补出来的 days_imp 值的平均数（左图）或标准差（右图）。
    横轴：迭代次数 1→10；纵轴：插补值的统计量（mean 或 sd）。
    收敛判定：如果几条链的曲线在几次迭代后都“纠缠到一块”并在某个水平线上左右波动，就说明 PMM 算法已经趋于稳定、不再大幅跳动。
    图里，Mean 大约在 20 附近、SD 在 6–7 附近徘徊；从迭代 4–5 开始，各链的轨迹都没出现明显的“下山”或“上升”趋势，说明已经收敛。

  2.6.4.小结:
    缺失分布：约 74%（152 012/206 209）的用户需要插补，25% 有真实观测。
    插补流程：MICE 在 5 条链上跑了 10 次迭代，每次都更新一次 days_imp。
    收敛情况：从诊断图看，各链在第 4、5 次左右就稳定下来了，m=5, maxit=10 的设置是足够的


3. 插补后特征重建
对每个插补版本，恢复 days_since_prior_order，并按原流程重建 cum_days、order_date 等特征
3.1 对于MI 使用了 订单级 MI  [1-3 步骤]，选用下面的代码以完整恢复 `cum_days`、`order_date`，可视化更灵活，但耗时更长
```{r Feature reconstruction orderLevel, warning=FALSE, message=FALSE}
# 3.1 拿到所有插补后的完整订单级数据集
completed_orders <- complete(imp_opt, "all")  # 或者 complete(imp, "all")

# 3.2 针对每个插补版本重建 cum_days、order_date
orders_reconstructed <- map_dfr(
  seq_along(completed_orders),
  function(i) {
    completed_orders[[i]] %>%
      # 把 days_imp 重命名回 days_since_prior_order
      rename(days_since_prior_order = days_imp) %>%
      arrange(user_id, order_number) %>%
      group_by(user_id) %>%
      mutate(
        days0    = replace_na(days_since_prior_order, 0),
        cum_days = cumsum(days0),
        # 如果需要伪日历：
        # anchor_date = cutoff_date - max(cum_days),
        # order_date  = anchor_date + cum_days
      ) %>%
      ungroup() %>%
      mutate(imputation = i)
  }
)

# 示范查看第一个 imputation 的前几行
orders_reconstructed %>% filter(imputation == 1) %>% slice_head(n = 10) %>% print()

```

3.2 对于MI 使用了 用户级 MI  [用户粒度聚合 步骤4]，选用下面的代码以完整恢复 `cum_days`、`order_date`，可视化更灵活，但耗时更长
```{r Feature reconstruction userLevel, warning=FALSE, message=FALSE}
# 3.1 拿到所有插补后的 user‐level 数据集
completed_list <- complete(imp_user, "all")

# 3.2 对每个插补版本，生成用户级特征表
user_features_mi <- map_dfr(
  seq_along(completed_list),
  function(i) {
    completed_list[[i]] %>%
      rename(
        # days_imp 就是 “最后一次下单间隔”
        recency_days = days_imp
      ) %>%
      mutate(
        # frequency 已经在聚合时计算过
        imputation = i
      ) %>%
      select(user_id, recency_days, frequency, imputation)
  }
)

# 看一下前几行
user_features_mi %>% group_by(imputation) %>% slice_head(n = 5) %>% print()
```

这样就得到了一个 **(user_id, recency_days, frequency, imputation)** 的长表，可以用于后续模型训练时对不确定性进行量化

4.验证 MI 后的插补质量
4.1 检查原始 vs 插补值的分布
4.2 查看每条插补链的轨迹（收敛 vs 不收敛）
4.3 对最终特征（recency_days）在不同 imputation 之间做对比


通过以下三方面的检查，可以对 MI 后的数据质量有较为全面的理解，确定是否需要调整 `m`、`maxit`、`predictorMatrix` 或插补方法

# 1. MICE 内置诊断：观察插补链 (days_imp) 的分布收敛情况, 密度图 & 条状图 可以直观对比：各条链是否在类似范围内插补，是否收敛
   - 如果各条链（颜色不同的线或不同 .imp 值）在大致同一区间聚拢，且密度曲线形状相似，说明插补模型稳定、收敛良好。  
   - 如果有某一条链与其它链明显分离，或曲线相差很大，说明可能需要增加 `maxit` 或检查 predictorMatrix。
   
```{r MI_Diagnostics_1a, warning=FALSE, message=FALSE}
# 1a) 密度图：各链最终插补值分布
densityplot(imp_user, ~ days_imp, main = "Density of Imputed days_imp by Chain")

```
1.a Density of Imputed days_imp by Chain
每一条彩色曲线代表一条插补链（imp = 1…5）对所有需要插补的 days_imp 值做出的“边际”分布
理想情况：各条曲线都在同一个多峰形状上来回（峰的位置相同、相对高度相似），说明无论哪条链，最终插补出的值都遵循同一分布，插补过程已收敛

该图：
- 可以看到一系列在 1–29 天之间的多重峰──对应用户常见的复购间隔（如 14、21、28 天等）只是高度和轮廓略有差异
- 虽然有少数链在 “4 天”处出现一个小峰，这往往是由于原始数据里确实存在一些 4 天间隔的订单（虽然不那么显眼），PMM 将它们当作候选“近邻”来插补
- 各链的峰位大体一致（比如都在 7、14、21、28 天出现高密度），虽然高度略有差别，但峰的位置对齐，说明链已收敛到同一多峰分布上

结论：插补链分布一致性良好，PMM 没跑出奇异模式。


```{r MI_Diagnostics_1b, warning=FALSE, message=FALSE}
# 1b) 条状图 (stripplot)：各条链在每次迭代的插补值分布
stripplot(imp_user, days_imp ~ .imp, vertical = TRUE,
          pch = 20, cex = 1.2,
          main = "Stripplot of days_imp by Imputation")

```
1.b Stripplot of days_imp by Imputation
横坐标是插补编号 1–5，纵坐标是每条插补生成的 days_imp 值。红色点代表“被插补”的值（原先的 30 被标记为 NA），蓝色点代表原本就观测到的真实值（因我们把它们也留在 long format 中一起画了）
理想情况：每个插补编号下的红点（插补值）应大致堆在同一垂直带，分布与其它编号相似，且主要覆盖那些观测不到的高间隔区段

该图：
- 红点（插补）主要落在 1–29 范围，与蓝点共存，并没有成片地跑到某个离谱区间，且各组横向高度和密度相仿
- 虽然偶有少数插补链插出的极端点略多，但整体看，5 组插补都在类似区间随机分布
- 那个插补编号 5 在 4 天处多几个红点，是正常的个别差异——只要大多数红点都落在合理区间，且与不同插补的红点分布相似，就说明 PMM 插补行为一致

结论：插补值在各次插补中并未“飘”到完全不同的区间，连贯性良好




# 2. 比较 Observed vs Imputed values
   - 原始观测 (`.imp == 0`) 与所有插补值 (`.imp > 0`) 应该总体分布相近但不完全重合。  
   - 如果插补值显著偏离原始观测（例如插补都集中在一个小区间），说明模型可能欠拟合或 predictorMatrix 太简单。

```{r MI_Diagnostics_2, warning=FALSE, message=FALSE}
# 2a) 把 imp_user 展开成长格式
long_imp <- complete(imp_user, "long", include = TRUE)

# 2b) 标记“observed”(原始) vs “imputed”
long_imp <- long_imp %>%
  mutate(type = ifelse(.imp == 0, "observed", "imputed"))

# 2c) 画 density 比较
ggplot(long_imp, aes(x = days_imp, fill = type)) +
  geom_density(alpha = 0.3) +
  labs(title = "Observed vs Imputed days_imp Distribution",
       x = "days_imp", fill = "")

```
2.c Observed vs Imputed days_imp Distribution
青色 (observed)：原始 days_imp（即真实 <30 的那部分）分布
粉色 (imputed)：插补出来的值分布。
合理的插补应当“填补” 观测稀疏的区间（在原本青色稀疏的区间，例如 10–12，16-20，23-26 天区间，带来更多密度），同时保留总体多峰结构（在主要观测峰周围延展）

该图：
- 看到青色在 0–30 天区间有好几段高峰（真实数据偏好在某几天复购）
- 粉色则在这些峰周围也出现对应频次，并在某些青色空白区（比如 8 天左右）增加了小峰──这正是 PMM 从相似观测中匹配出来的结果
- 在 4 天等次要区间出现小峰，只要它不是过度主导分布，就不必担心

结论：插补分布符合预期，既不硬贴在原始青色分布上，也没跑到荒唐区间



# 3. 比较 recency_days 在各 imputation 之间的一致性
   - 查看 `mean_recency`、`sd_recency` 在各个 imputation 之间的差异，如果差异很小（小于业务可接受范围），说明 MI 不确定性较低
   - Density/boxplot 可以直观对比分位和尾部差异
   
```{r MI_Diagnostics_3a, warning=FALSE, message=FALSE}
# 3a) 汇总 user_features_mi
mi_stats <- user_features_mi %>%
  group_by(imputation) %>%
  summarise(
    mean_recency   = mean(recency_days),
    sd_recency     = sd(recency_days),
    median_recency = median(recency_days),
    min_recency    = min(recency_days),
    max_recency    = max(recency_days),
    .groups = "drop"
  )

print(mi_stats)

```
解读：
mean_recency：每次插补计算出的全体用户 recency_days 的平均值。
sd_recency：其标准差，衡量插补后分布的离散程度。
median_recency、min_recency、max_recency：分别是中位数、最小值和最大值。

可以注意到，每次插补的 mean_recency 在 ~16–23 天之间变动，sd_recency 在 ~4–8 天之间。这反映了多重插补带来的不确定性：我们并不只是填一个固定值，而是在合理区间内生成多种可能，方便后续在模型或报告中量化这部分不确定性。


```{r MI_Diagnostics_3b, warning=FALSE, message=FALSE}
# 3b) 可视化各 imputation 的 recency 分布
ggplot(user_features_mi, aes(x = recency_days, color = factor(imputation))) +
  geom_density() +
  labs(title = "Recency_days Density by Imputation",
       x = "recency_days", color = "imp #") +
  theme_minimal()

```


```{r MI_Diagnostics_3c, warning=FALSE, message=FALSE}
# 3c) 或者 boxplot
ggplot(user_features_mi, aes(x = factor(imputation), y = recency_days)) +
  geom_boxplot() +
  labs(title = "Recency_days Boxplot by Imputation",
       x = "Imputation", y = "recency_days") +
  theme_minimal()
```
这两张图对比了每次插补后、用户级特征 recency_days（最后一次间隔）的差异
- Density：五条彩线表示五次插补的密度曲线。它们虽然峰位置和高度略有偏移（比如第 3 次峰更多集中在 15–17 天，第 2 次在 ~23 天），但总体范围一致。
- Boxplot：展示每次插补的中位数（盒子中线）、四分位距（盒子高度）及离群点。可以看到：
    中位数在 13–24 天浮动
    IQR（盒子）大约在 15–25 天左右

结论：插补带来了大约 ±4 天的全局均值波动和 ±2–3 天的四分位距变化，但没有出现某次插补完全脱离其他四次的情况。



总体评价
- 虽然看上去“第 3 次”和“第 4 次”插补的均值偏低（~16 天），“第 2 次”偏高（~23 天），但这种 ±4–6 天的波动正是 MI 希望保留的“插补不确定性”。
- 没有任何一条插补链或一次插补的分布彻底跑偏到 0–5 天或 25–30 天区间之外，说明没有野值或模式崩坏。
- 因此，当前 MI 结果是合格且可用的。若仍担心波动范围，可以：
    a. 增大 m（从 5 → 7 或 10），让波动估计更稳定；
    b. 加入更多协变量（如 order_dow）到 predictorMatrix 中，提升插补精度；
    c. 切分用户群（高频 vs 低频）分别插补，以捕捉不同行为模式。

---
监控与告警
1. 监控缺失 / 截断比例
- 缺失比例：指在每批次（或每天、每周）的 orders 表里，days_since_prior_order 为 NA（首单）的行数占总行数的比重。
- 截断比例：指该字段恰好等于 30 天（即被硬截断）的订单数占总行数的比重。

2. 为什么要设置监控
- 数据质量预警：如果有一天缺失比例突然从 ~6% 飙高到 20%，说明 ETL 可能出了问题（字段没被正确填 0）；
- 业务模式变动：截断比例突然升高，可能后台把阈值从 30 改成别的数，或客户行为出现剧变，需要核实。
- 模型健壮性robustness：截断分布一旦大幅变化，对插补及模型预测都会产生影响，及时发现能及时回滚或重新训练。

3. 手段
1. 在 Athena 里建一个定期跑的视图或表

2. 在 QuickSight 或 Athena 控制台建一个 Time-series 图表
  - 把 missing_cnt/total_cnt 和 trunc_cnt/total_cnt 当作两条折线监控
  - 设置阈值告警：例如当某日任一比例超过 15%，邮件/Slack 提醒我们

---
敏感性分析与报告

1. MNAR 情境模拟（Best/Worst‐case）
目标：评估“截断为 30 天”这一假设对特征和模型结果的影响

1.1 Best‐case & Worst‐case 场景
Best‐case（最乐观）：假定所有被截断的间隔都恰好是阈值（30 天）
Worst‐case（最悲观）：假定所有被截断的实际间隔都远大于阈值（例如 60 天或 90 天）

1.2 代码
```{r Best_Worst_case, warning=FALSE, message=FALSE}
# 先定义一个函数：给定 days_sc（插补或假设后的间隔）重算用户级 cum_days & recency_days
recalc_features <- function(df) {
  df %>%
    arrange(user_id, order_number) %>%
    group_by(user_id) %>%
    mutate(cum_days = cumsum(days_sc)) %>%
    summarise(recency_days = max(cum_days), .groups="drop")
}

# Best-case：保留 30 天截断
orders_best <- orders_rel %>%
  mutate(days_sc = replace_na(days_since_prior_order, 0))  # NA→0, 30 保持 30

# Worst-case：把所有 30 天截断值统一当作 60 天
orders_worst <- orders_rel %>%
  mutate(days_sc = replace_na(days_since_prior_order, 0),
         days_sc = if_else(days_sc == 30, 60, days_sc))

# 重算特征并打标签
feat_best  <- recalc_features(orders_best)  %>% mutate(scenario = "best")
feat_worst <- recalc_features(orders_worst) %>% mutate(scenario = "worst")

# 合并
feat_bw <- bind_rows(feat_best, feat_worst)

# 可视化 recency_days 的分布
ggplot(feat_bw, aes(x = recency_days, fill = scenario)) +
  geom_density(alpha = 0.3) +
  labs(
    title = "Sensitivity: Best vs Worst",
    x     = "Recency Days",
    fill  = "Scenario"
  ) +
  theme_minimal()

```
这张“Best vs Worst”敏感性密度对比图清晰地展示了在两种极端假设下，用户最后一次下单距首单（recency_days）的分布有多么截然不同

1. 粉色（Best‐case）曲线
假设：所有原本被截断为 30 天的间隔，就都真的只有 30 天
分布特点：
  - 主峰集中在较低的 recency_days 值（大约在 50–120 天区间内，具体峰值取决于数据）
      最高点大约落在 60–80 天，说明如果所有 “≥30 天” 都当成 30 天，那么多数用户从首单到最后一单的间隔在两三个月内完成
  - 主峰的右侧隐约可以看到几个小峰，间隔大约 20–30 天，反映了 7 天周周期的叠加效应
  - 350 附近有次峰，但小于Worst‐case 的曲线
  - 长尾峰在 ~350 天处 那个右侧的尖峰估计是对应那些订单数特别多的用户，他们即使每次都按 30 天算，也累积到近一年才完成所有订单

2. 青绿色（Worst‐case）曲线
假设：所有被截断的间隔都实际是 60 天 （将截断的 30 天都当作 60 天）
分布特点:
  - 主峰右移到更大的 recency_days 值（大约在 350–400 天区间）说明如果每次“≥30 天”都记作 60 天，绝大多数用户看起来近一年才完成他们的最后一单
  - 在 150–300 天之间有几个小峰，对应那些只有少数几次“超长”间隔的用户（如第 2、3 单出现一次 60 天后，第 3、4 单又是常规 7 天左右）

3. 对比与启示
3.1 峰值位置彻底不同
  - Best_case：主峰在 ~80 天；Worst‐case：主峰在 ~400 天，左右差异达数百天。

3.2 周周期波动仍保留
  - 虽然在 Worst_case 下，复购的周周期（7 天倍数）畸变得不那么明显，但仍可在中段小峰中看到约 20–30 天、90 天等规律

3.3 业务解读
  - 如果实际用户确实在很多订单里出现了 30 天以上的长间隔，那么把它们当成 30 天低估了流失期；反之全部当成 60 天又可能高估
  - 这提示我们，单一截断假设对 recency_days 的影响极大，需要用多重插补或灵活分箱来减缓这种偏差

3.4 改进
  - 考虑分层截断：对高频用户（frequency 高）的截断值设小一些（如 30），对低频用户设大一些（如 60），以更贴近不同用户的真实行为
  - 在模型里采用分箱特征：把 recency_days 离散到“<60 天”、“61–180 天”、“>180 天”这类分箱，减少对具体数值的敏感度
  - 保留插补不确定性：利用 MI 后得到的多套 recency_days，在模型预测时输出区间预测而非点预测，为决策提供可靠的可信区间
  
---
2. Delta‐adjustment
目标：测试“截断真实值比 30 天多些或少些”对模型的影响。
通过查看 mean_recency、median_recency 随 delta 变化的曲线斜率，评估对特征的敏感度
```{r Delta_adjustment, warning=FALSE, message=FALSE}
# 定义同一个 recalc_features 函数（同Best_Worst_case，如果还没定义）
# # 定义一系列 Delta，表示给截断值加上的偏移
deltas <- c(-5, 0, 5, 15)   # 比如：-5, 0, +5, +15 天

# 对每个 delta 生成一个场景
feat_delta <- map_dfr(deltas, function(d) {
  orders_rel %>%
    mutate(
      days_sc = replace_na(days_since_prior_order, 0),
      days_sc = if_else(days_sc == 30, days_sc + d, days_sc),
      days_sc = pmax(days_sc, 0)   # 防止负值
    ) %>%
    recalc_features() %>%
    mutate(delta = d)
})

# 计算每个 delta 下的统计量
delta_stats <- feat_delta %>%
  group_by(delta) %>%
  summarise(
    mean_rec    = mean(recency_days),
    median_rec  = median(recency_days),
    p25         = quantile(recency_days, 0.25),
    p75         = quantile(recency_days, 0.75),
    .groups     = "drop"
  )

# 绘制 Mean & Median & IQR 随 delta 变化
ggplot(delta_stats, aes(x = delta)) +
  geom_line(aes(y = mean_rec,    color = "Mean"),    size = 1) +
  geom_point(aes(y = mean_rec,   color = "Mean"),    size = 2) +
  geom_line(aes(y = median_rec,  color = "Median"),  linetype = "dashed", size = 1) +
  geom_point(aes(y = median_rec, color = "Median"),  shape = 17, size = 2) +
  geom_ribbon(aes(ymin = p25, ymax = p75), fill = "grey70", alpha = 0.4) +
  labs(
    title = "Delta-adjustment Sensitivity",
    x     = "Delta applied to truncated values",
    y     = "Recency Days",
    color = ""
  ) +
  scale_color_manual(values = c("Mean" = "blue", "Median" = "red")) +
  theme_minimal()

```

1. Mean（实线，蓝色）呈线性上移
  - 当 Δ 从 −5 → 0 → 5 → 15 时，平均 recency_days 大致也从 ~165 → ~175 → ~185 → ~200线性上升
  - 这说明对所有“30 天”值加 Δ，整体平均会被拉高 Δ 天左右（略有微小偏差，但非常接近一对一关系）

2. Median（虚线，红色）同样呈线性上移
  - 中位数从 ~140 → ~150 → ~160 → ~185 左右随 Δ 上移。
  - 和平均类似，只不过因为中位数对极端值不敏感，它的上升幅度有时会稍小或稍大于 Δ，但总体仍是单调、近线性的

3. IQR（灰色带）灰色阴影带代表各个 Δ 下 25%–75% 分位点 的区间，大致保持常宽，整体上移
  - 当 Δ 改变时，阴影带整体 往上平移
  - 带的 高度（宽度）几乎没变，说明分位距（IQR）保持稳定，分布形状未被扭曲

小结：
1. 可控的平移：只要给截断值加 Δ，recency_days 的集中趋势就会稳健地、几乎等量上移，便于做参数校准或业务对齐
2. 分布形状不变：IQR 保持近乎恒定，表示无论怎么假设，用户群的内部分布“宽度”／“离散度”都差不多，模型中对相对顺序的判断不会乱
3. 这张图展示了对“截断值假设”的完全可控敏感度：（对于后续的模型校准、阈值设置）只要 Δ 调整量给定，Recency 的中心位置（Mean/Median）就跟着直线级上移，而分布宽度保持稳定



---

3. 报告透明披露
在报告或 Confluence 文档中，务必要写明：
1. 哪些字段为什么要插补（此处：days_since_prior_order == 30 属于 MNAR 截断）
2. 使用的插补方法与参数
  - 方法：Predictive Mean Matching
  - m 值：5 （后续可以测试 7或10）
  - maxit：10
  - 协变量：frequency 等

如何检验 MNAR（前面做过的相关性检验和可视化说明）

敏感性测试

Best/Worst‐case 和 Delta‐adjustment 的流程与主要结论

“如果假设改变，模型输出如何”

---
5. 相关性与聚类
```{r corrplot, warning=FALSE, message=FALSE}
# 相关性矩阵
num_df <- orders %>% select_if(is.numeric)
corr_mat <- cor(num_df, use="pairwise.complete.obs") # 计算每一对数值列的皮尔逊相关系数，自动跳过缺失值。结果是一个 5×5 的对称矩阵，元素值在 –1 到 +1 之间
corrplot::corrplot(corr_mat, method="ellipse") # 用椭圆的形状和颜色深浅来可视化相关系数： 深蓝＆细长 表示强正相关（接近 +1）；深红＆扁宽 表示强负相关（接近 –1）；浅灰/扁圆 表示无或弱相关（接近 0）
```
order_number 与 user_id：正相关很高（因为每个用户的订单序号从 1 开始自增），这完全是数据结构决定的。
order_number 与 days_since_prior_order：轻微负相关，说明用户下单次数越多，平均每次下单间隔往往越短（忠诚用户复购更频繁）。
其他组合同样反映了“用户活跃度”（高 order_number ↔ 低 days_since_prior_order）等直观关系。


```{r K-Means Clustering, warning=FALSE, message=FALSE}
# 用户聚类
cluster_df <- rfm %>% select(recency, frequency) %>% scale()
set.seed(42)
km <- kmeans(cluster_df, centers=4)
rfm$cluster <- factor(km$cluster)

rfm %>% 
  # 去掉可能的 NA
  filter(!is.na(recency) & !is.na(frequency) & !is.na(cluster)) %>% 
  ggplot(aes(x = frequency, y = recency, color = cluster)) +
    geom_jitter(alpha = 0.4, width = 0.2, height = 0.2, size = 1.5) +
    scale_y_reverse() +   # recency 越小越近期，倒转 Y 轴更直观
    scale_x_continuous(labels = scales::comma) +
    labs(
      title    = "R–F 用户聚类分布",
      subtitle = "每个点代表一个用户，颜色表示 K-means 分群",
      x        = "Frequency (总订单数)",
      y        = "Recency (距今天数，倒序)",
      color    = "Cluster"
    ) +
    theme_minimal() +
    theme(
      legend.position = "right",
      plot.subtitle   = element_text(size = 10)
    )
```





之前的RFM是用分位数分箱 (ntile) 的方法绘制 R 和 F 网格
按新近度对用户进行排序，并为每个指标将其划分为 **5 个大小相等的桶**（分位数），然后绘制 R_score 与 F_score 的关系图。
**优点：**非常透明且易于解释（“此单元格是最近前 20% 的数据，也是最常出现的前 20% 的数据”）。
**缺点：** 1.即使数据分布不平衡，也会在第 20、40、60、80 个百分位数处施加**严格的截断**。 2.不考虑联合分布的“形状”；单独处理每个轴。

K 均值聚类 — “基于距离的细分”
将两个指标标准化（均值 0，标准差 1），然后运行 ​​K 均值聚类将用户划分为 **4 个聚类**，以最小化二维 R-F 空间中的聚类内方差。
**输出：** 四个组，每个组都有各自的质心（例如“高频/高频”、“低频/中频”等）。
**优点：** 1.聚类基于**实际数据密度**和自然分组，而非任意百分位数。 2.可以根据业务需求选择任意数量的聚类。
**缺点：** 1.如果利益相关者期望“前 20% = VIP”的逻辑，则更难解释。 2.必须事后解释每个聚类的含义（例如，聚类 1 =“新手”，聚类 2 =“潜在高消费用户”）。

如果认为客户构成是**自然群体**（例如，与 20% 的削减不一致的中频“最佳点”），并且希望数据能够驱动细分定义，那么**K 均值**会是更好的选择。

散点图“太”线性了，因为**新近度**计算方式是：recency = as.numeric(today() - max(order_number)) 实际上，它的作用是 recency_i = today() - (用户 i 的最高 order_number) 所以，如果用户 i 下了 50 个订单，就是在执行 `today() - 50`——订单数量和“新近度”之间完美的一一对应关系，因此是直线对角线

修复“最近成交量”→使用自上次订单以来的实际天数

```{r K-Means Clustering2, warning=FALSE, message=FALSE}
library(forcats)
# 1. Build pseudo-calendar orders_fake first (if not already in env)
base_date <- as.Date("2024-01-01")

orders_fake <- orders %>% # # 为每个订单重建一个伪日历日期
  arrange(user_id, order_number) %>%
  group_by(user_id) %>%
  mutate(
    days       = replace_na(days_since_prior_order, 0),
    cum_days   = cumsum(days),
    order_date = base_date + cum_days
  ) %>%
  ungroup()

# 2. Compute RFM properly from orders_fake
rfm2 <- orders_fake %>%
  group_by(user_id) %>%
  summarise(
    recency   = as.numeric(today() - max(order_date)),  # 自上次实际 order_date 以来的天数
    frequency = n(),                                    # 订单总数
    .groups   = "drop"
  )

# 3. K-means clustering
cluster_df <- rfm2 %>% select(recency, frequency) %>% scale()
set.seed(42)
km <- kmeans(cluster_df, centers = 4)
rfm2$cluster <- factor(km$cluster)

# 4. Plot the clusters in RF space
rfm2 %>%
  ggplot(aes(x = frequency, y = recency, color = cluster)) +
    geom_jitter(alpha = 0.4, size = 1.5, width = 0.2, height = 0.2) +
    scale_y_reverse() +  
    scale_x_continuous(labels = scales::comma) +
    labs(
      title    = "R–F 用户聚类分布（Recency = days since last order）",
      x        = "Frequency (总订单数)",
      y        = "Recency (天, 倒序)",
      color    = "Cluster"
    ) +
    theme_minimal()

```

1. **`order_date` 是一个真正的日历日期**（尽管是伪日历），通过累计真实的 `days_since_prior_order` 偏移量。
2. **`recency = today() - max(order_date)`** 现在衡量的是自用户上次下单以来的实际天数。
3. 由于 **频率**（订单总数）和 **新近度**（自上次下单以来的天数）现在是 **两个独立的真实世界信号**，因此它们之间不存在强制的线性关系。


聚类颜色 新近度 频率 业务解读
1 🔴 红色 高 低 风险/休眠
订单总数很少，并且很长时间没有购买。适合积极地进行挽回营销活动。
2 🟢 绿色 低 高 冠军
下单频率很高，并且刚刚购买——您最忠诚、价值最高的客户。重点关注 VIP 权益。
3 🔵 蓝绿色 中 低-中 潜力
下单频率适中，并且距离上次下单时间不长。可以通过有针对性的优惠活动将其转化为常客。
4 🟣 紫色 低-中 中-高 忠诚但老化
下单频率不错，但很久没有下单了。他们正在流失——提醒邮件或新的激励措施可以重新激活他们。
